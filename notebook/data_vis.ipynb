{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d59061",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20014dad",
   "metadata": {},
   "source": [
    "# 0) Process data\n",
    "\n",
    "We want to define :\n",
    "- Definition of  default\n",
    "- Time step (12 months, 24 months)\n",
    "\n",
    "If the lender default after 24 months we do not take it into account.\n",
    "\n",
    "\n",
    "We define the default as having 90 days of non payment or having zero_balance code a 02, 03 or 09 (cf data details) between the \"FIRST PAYMENT DATE\" and the \"MONTLHLY REPORTING PERIOD\" being under 12 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### make_labels.py initial code\n",
    "\n",
    "# Paths to origination and performance datasets\n",
    "path_orig = \"../data/raw/mortgage_data/historical_data_2022Q1/historical_data_2022Q1.txt\"\n",
    "path_perf = \"../data/raw/mortgage_data/historical_data_2022Q1/historical_data_time_2022Q1.txt\"\n",
    "\n",
    "# Column names for the Origination dataset\n",
    "colnames_origination = [\n",
    "    \"credit_score\",                         # Borrower credit score\n",
    "    \"first_payment_date\",                   # First scheduled payment date (YYYYMM)\n",
    "    \"first_time_homebuyer_flag\",            # First-time homebuyer flag\n",
    "    \"maturity_date\",                        # Loan maturity date (YYYYMM)\n",
    "    \"msa_md\",                               # MSA / Metropolitan Division\n",
    "    \"mi_percent\",                           # Mortgage Insurance percentage\n",
    "    \"number_of_units\",                      # Number of units in the property\n",
    "    \"occupancy_status\",                     # Occupancy status (owner/second/investment)\n",
    "    \"original_cltv\",                        # Original Combined Loan-to-Value\n",
    "    \"original_dti\",                         # Original Debt-to-Income ratio\n",
    "    \"original_upb\",                         # Original Unpaid Principal Balance\n",
    "    \"original_ltv\",                         # Original Loan-to-Value\n",
    "    \"original_interest_rate\",               # Original Interest Rate\n",
    "    \"channel\",                              # Origination channel (Retail, Broker, etc.)\n",
    "    \"ppm_flag\",                             # Prepayment penalty flag\n",
    "    \"amortization_type\",                    # Amortization type (FRM/ARM)\n",
    "    \"property_state\",                       # Property state (2-letter code)\n",
    "    \"property_type\",                        # Property type\n",
    "    \"postal_code\",                          # Postal code (last 2 digits = 00)\n",
    "    \"loan_sequence_number\",                 # Unique loan identifier (primary key)\n",
    "    \"loan_purpose\",                         # Loan purpose (Purchase, Refinance)\n",
    "    \"original_loan_term\",                   # Original loan term (months)\n",
    "    \"number_of_borrowers\",                  # Number of borrowers\n",
    "    \"seller_name\",                          # Seller name\n",
    "    \"servicer_name\",                        # Servicer name\n",
    "    \"super_conforming_flag\",                # Super conforming flag\n",
    "    \"pre_relief_refi_loan_seq_number\",      # Pre-relief refinance loan sequence number\n",
    "    \"special_eligibility_program\",          # Special eligibility program\n",
    "    \"relief_refinance_indicator\",           # Relief refinance indicator\n",
    "    \"property_valuation_method\",            # Property valuation method\n",
    "    \"interest_only_indicator\",              # Interest-only indicator\n",
    "    \"mi_cancellation_indicator\"             # MI cancellation indicator\n",
    "]\n",
    "\n",
    "# Column names for the Performance dataset\n",
    "colnames_performance = [\n",
    "    \"loan_sequence_number\",                 # Loan identifier (primary key)\n",
    "    \"monthly_reporting_period\",             # Reporting period (YYYYMM)\n",
    "    \"current_actual_upb\",                   # Current actual Unpaid Principal Balance\n",
    "    \"current_loan_delinquency_status\",      # Loan delinquency status (0,1,2,..., RA)\n",
    "    \"loan_age\",                             # Loan age in months\n",
    "    \"remaining_months_to_legal_maturity\",   # Remaining months until legal maturity\n",
    "    \"defect_settlement_date\",               # Defect settlement date (YYYYMM)\n",
    "    \"modification_flag\",                    # Loan modification flag\n",
    "    \"zero_balance_code\",                    # Zero balance code (01,02,03,09, etc.)\n",
    "    \"zero_balance_effective_date\",          # Zero balance effective date (YYYYMM)\n",
    "    \"current_interest_rate\",                # Current interest rate\n",
    "    \"current_non_interest_bearing_upb\",     # Current non-interest-bearing UPB\n",
    "    \"ddlpi\",                                # Due Date of Last Paid Installment (YYYYMM)\n",
    "    \"mi_recoveries\",                        # Mortgage insurance recoveries\n",
    "    \"net_sale_proceeds\",                    # Net sale proceeds\n",
    "    \"non_mi_recoveries\",                    # Non-MI recoveries\n",
    "    \"total_expenses\",                       # Total expenses\n",
    "    \"legal_costs\",                          # Legal costs\n",
    "    \"maintenance_and_preservation_costs\",   # Maintenance & preservation costs\n",
    "    \"taxes_and_insurance\",                  # Taxes and insurance\n",
    "    \"miscellaneous_expenses\",               # Miscellaneous expenses\n",
    "    \"actual_loss_calculation\",              # Actual loss calculation\n",
    "    \"cumulative_modification_cost\",         # Cumulative modification cost\n",
    "    \"step_modification_flag\",               # Step modification flag\n",
    "    \"payment_deferral\",                     # Payment deferral indicator\n",
    "    \"estimated_ltv\",                        # Estimated Loan-to-Value (ELTV)\n",
    "    \"zero_balance_removal_upb\",             # Zero balance removal UPB\n",
    "    \"delinquent_accrued_interest\",          # Delinquent accrued interest\n",
    "    \"delinquency_due_to_disaster\",          # Delinquency due to disaster\n",
    "    \"borrower_assistance_status_code\",      # Borrower assistance status code\n",
    "    \"current_month_modification_cost\",      # Current month modification cost\n",
    "    \"interest_bearing_upb\"                  # Interest-bearing UPB\n",
    "]\n",
    "\n",
    "# Load both datasets (pipe-delimited, no header)\n",
    "df_train_train_orig = pd.read_csv(path_orig, sep=\"|\", header=None, names=colnames_origination)\n",
    "df_perf = pd.read_csv(path_perf, sep=\"|\", header=None, names=colnames_performance)\n",
    "\n",
    "\n",
    "# Shape data\n",
    "print(f\"Shape data orig : {df_orig.shape}\")\n",
    "print(f\"Shape data perf: {df_perf.shape}\")\n",
    "\n",
    "# Convert YYYYMM to datetime\n",
    "df_perf[\"monthly_reporting_period\"] = pd.to_datetime(\n",
    "    df_perf[\"monthly_reporting_period\"].astype(str), \n",
    "    format=\"%Y%m\"\n",
    ")\n",
    "df_orig[\"first_payment_date\"] = pd.to_datetime(\n",
    "    df_orig[\"first_payment_date\"].astype(str), \n",
    "    format=\"%Y%m\"\n",
    ")\n",
    "\n",
    "# Convert to monthly Period type (YYYY-MM), easier for month arithmetic\n",
    "df_perf[\"monthly_reporting_period\"] = df_perf[\"monthly_reporting_period\"].dt.to_period(\"M\")\n",
    "df_orig[\"first_payment_date\"] = df_orig[\"first_payment_date\"].dt.to_period(\"M\")\n",
    "\n",
    "# Merge Origination info into Performance dataset\n",
    "df_perf = pd.merge(\n",
    "    df_perf, \n",
    "    df_orig[[\"loan_sequence_number\", \"first_payment_date\"]], \n",
    "    on=\"loan_sequence_number\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Compute loan age in months since origination\n",
    "df_perf[\"months_since_orig\"] = (\n",
    "    df_perf[\"monthly_reporting_period\"] - df_perf[\"first_payment_date\"]\n",
    ").apply(lambda x: x.n)\n",
    "\n",
    "# Flag observations within the first 24 months after origination\n",
    "df_perf[\"within_24m\"] = df_perf[\"months_since_orig\"] <= 24\n",
    "\n",
    "\n",
    "\n",
    "df_perf_within = df_perf.copy()\n",
    "# print(df_perf_within.shape)\n",
    "df_perf_within = df_perf_within[df_perf_within[\"within_24m\"] == True]\n",
    "# print(df_perf_within.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define default:\n",
    "# - delinquency status not in {0,1,2} (=> 90+ days delinquent or RA)\n",
    "# - OR zero_balance_code in {03,09,15,16,96} (dispositions / foreclosures)\n",
    "df_perf_within[\"default\"] = np.where(\n",
    "    (~df_perf_within[\"current_loan_delinquency_status\"].astype(str).isin([\"0\",\"1\",\"2\"])) |\n",
    "    (df_perf_within[\"zero_balance_code\"].astype(str).isin([\"03\",\"09\",\"15\",\"16\",\"96\"])),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "# Aggregate at loan level: max(default) = 1 if loan ever defaulted in 24m\n",
    "loan_level = (\n",
    "    df_perf_within.groupby(\"loan_sequence_number\")[\"default\"]\n",
    "    .max()  # 0 if always current, 1 if ever default\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_orig = pd.merge(df_orig, loan_level, on = \"loan_sequence_number\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72b68b",
   "metadata": {},
   "source": [
    "# 0) Working on the data\n",
    "\n",
    "\n",
    "When building a credit scoring model, it’s essential to respect the time dimension: the model should be trained on past vintages of loans and evaluated on future vintages. For example, if we have data for 2021Q4, 2022Q1, and 2022Q2, you would train on the first two quarters and keep 2022Q2 strictly for testing. This “out-of-time” validation mimics the real-world situation where a model is always used to predict the future, and it ensures that performance and calibration are not artificially inflated by information leakage across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49fd99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "PATH = Path(\"../data/processed/default_labels\")\n",
    "PATTERN = re.compile(r\"default_labels_24m_(\\d{4}Q[1-4])\\.csv$\", re.I)\n",
    "\n",
    "# Optionnel: accélère et économise de la RAM (pandas ≥ 2.0)\n",
    "READ_KW = dict(engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n",
    "# Optionnel: ne charger que les colonnes utiles\n",
    "# READ_KW[\"usecols\"] = [\"loan_sequence_number\", \"default_24m\", \"vintage\", ...]\n",
    "\n",
    "files = sorted(\n",
    "    (p for p in PATH.glob(\"default_labels_24m_*.csv\") if PATTERN.match(p.name)),\n",
    "    key=lambda p: PATTERN.match(p.name).group(1)\n",
    ")\n",
    "\n",
    "buckets = {\"train\": [], \"validation\": [], \"test\": []}\n",
    "def year_to_split(y: int):\n",
    "    if 2020 <= y <= 2022: return \"train\"\n",
    "    if y == 2023:         return \"validation\"\n",
    "    if y == 2024:         return \"test\"\n",
    "    return None  # ignore/alerter si autre année\n",
    "\n",
    "for p in files:\n",
    "    qstr = PATTERN.match(p.name).group(1)       # ex: \"2021Q3\"\n",
    "    q = pd.Period(qstr, freq=\"Q\")\n",
    "    split = year_to_split(q.year)\n",
    "    if split is None:\n",
    "        print(f\"Ignoré: {p.name}\")\n",
    "        continue\n",
    "    df = pd.read_csv(p, **READ_KW)\n",
    "    df[\"vintage\"] = q                            # utile pour groupby/tri\n",
    "    buckets[split].append(df)\n",
    "\n",
    "# Concat une seule fois par split (plus rapide que concat successifs)\n",
    "df_train       = pd.concat(buckets[\"train\"], ignore_index=True)\n",
    "df_validation  = pd.concat(buckets[\"validation\"], ignore_index=True)\n",
    "df_test        = pd.concat(buckets[\"test\"], ignore_index=True)\n",
    "\n",
    "# (Optionnel) dédup si nécessaire\n",
    "# for d in (df_train, df_validation, df_test):\n",
    "#     d.drop_duplicates(subset=\"loan_sequence_number\", inplace=True)\n",
    "\n",
    "# Proportions jolies\n",
    "sizes = {\"train\": len(df_train), \"validation\": len(df_validation), \"test\": len(df_test)}\n",
    "total = sum(sizes.values())\n",
    "for k, n in sizes.items():\n",
    "    print(f\"{k.capitalize():<12}: {n:>10,} rows  ({n/total:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc1236",
   "metadata": {},
   "source": [
    "# 1) Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype, is_integer_dtype, is_bool_dtype\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LoanDataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encapsule les transformations de typage/parse/casts/catégories\n",
    "    pour df_train et df_validation, avec catégories apprises sur le train.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_col='default_24m'):\n",
    "        self.target_col = target_col\n",
    "\n",
    "        # Catégories \"fixes\" selon doc\n",
    "        self.fixed_cats_ = {\n",
    "            'occupancy_status': ['P','S','I'],\n",
    "            'channel': ['R','B','C','T'],\n",
    "            'amortization_type': ['FRM','ARM'],\n",
    "            'property_type': ['SF','CO','PU','CP','MH'],\n",
    "            'loan_purpose': ['P','C','N','R'],\n",
    "            'special_eligibility_program': ['H','F','R'],\n",
    "        }\n",
    "\n",
    "        # Ces catégories sont apprises sur le train (dynamiques)\n",
    "        self.learned_cats_ = {\n",
    "            'property_state': None,            # toutes les valeurs vues sur le train (upper)\n",
    "            'property_valuation_method': None, # valeurs Int8 vues sur le train (hors 9)\n",
    "        }\n",
    "\n",
    "    # ---------- Helpers ----------\n",
    "    @staticmethod\n",
    "    def _yn_space_to_bool(s: pd.Series, na_vals=('9',)):\n",
    "        s = s.astype('string').str.strip().str.upper()\n",
    "        # blank (espace/vide) = False par défaut\n",
    "        s = s.fillna('').replace({' ': ''})\n",
    "        out = s.map({'Y': True, 'N': False, '': False})\n",
    "        if na_vals:\n",
    "            out[s.isin(na_vals)] = pd.NA\n",
    "        return out.astype('boolean')\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_periodM(col: pd.Series):\n",
    "        s = col.astype('string').str.strip()\n",
    "        s = s.where(s.str.fullmatch(r'\\d{6}'), pd.NA)  # YYYYMM\n",
    "        return pd.PeriodIndex(s, freq='M')\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_periodQ(col: pd.Series):\n",
    "        s = col.astype('string').str.strip().str.upper()\n",
    "        s = s.where(s.str.fullmatch(r'\\d{4}Q[1-4]'), pd.NA)  # ex: 2016Q4\n",
    "        return pd.PeriodIndex(s, freq='Q')\n",
    "\n",
    "    # ---------- Fit ----------\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy()\n",
    "\n",
    "        # Pré-traitement minimal pour apprendre les catégories dynamiques\n",
    "        # property_state\n",
    "        if 'property_state' in df.columns:\n",
    "            s = df['property_state'].astype('string').str.strip().str.upper()\n",
    "            self.learned_cats_['property_state'] = sorted(s.dropna().unique().tolist())\n",
    "\n",
    "        # property_valuation_method (remplacer 9 -> NA, caster Int8, puis apprendre les niveaux)\n",
    "        if 'property_valuation_method' in df.columns:\n",
    "            pvm = pd.to_numeric(df['property_valuation_method'], errors='coerce')\n",
    "            pvm = pvm.replace({9: pd.NA}).astype('Int8')\n",
    "            self.learned_cats_['property_valuation_method'] = sorted(pd.Series(pvm).dropna().unique().tolist())\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ---------- Transform ----------\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "\n",
    "        # --- Dates / périodes ---\n",
    "        if 'first_payment_date' in df.columns:\n",
    "            df['first_payment_date'] = pd.to_datetime(df['first_payment_date'], errors=\"coerce\").dt.to_period(\"M\")\n",
    "        if 'maturity_date' in df.columns:\n",
    "            df['maturity_date'] = self._to_periodM(df['maturity_date'])\n",
    "        if 'vintage' in df.columns:\n",
    "            df['vintage'] = self._to_periodQ(df['vintage'])\n",
    "\n",
    "        # --- Booléens (selon doc) ---\n",
    "        if 'ppm_flag' in df.columns:\n",
    "            df['ppm_flag'] = self._yn_space_to_bool(df['ppm_flag'], na_vals=())\n",
    "        if 'interest_only_indicator' in df.columns:\n",
    "            df['interest_only_indicator'] = self._yn_space_to_bool(df['interest_only_indicator'], na_vals=())\n",
    "        if 'super_conforming_flag' in df.columns:\n",
    "            df['super_conforming_flag'] = self._yn_space_to_bool(df['super_conforming_flag'], na_vals=())\n",
    "        if 'first_time_homebuyer_flag' in df.columns:\n",
    "            df['first_time_homebuyer_flag'] = self._yn_space_to_bool(df['first_time_homebuyer_flag'], na_vals=('9',))\n",
    "        if 'relief_refinance_indicator' in df.columns:\n",
    "            # Y / (blank)\n",
    "            df['relief_refinance_indicator'] = self._yn_space_to_bool(df['relief_refinance_indicator'])\n",
    "\n",
    "        # Cible optionnelle -> boolean nullable\n",
    "        if self.target_col in df.columns:\n",
    "            s = df[self.target_col]\n",
    "            if is_integer_dtype(s) or is_bool_dtype(s):\n",
    "                df[self.target_col] = s.map({1: True, 0: False}).astype('boolean')\n",
    "\n",
    "        # --- Sentinelles -> NA puis cast num ---\n",
    "        if 'credit_score' in df.columns:\n",
    "            df['credit_score'] = df['credit_score'].replace({9999: pd.NA}).astype('Int16')\n",
    "        if 'mi_percent' in df.columns:\n",
    "            df['mi_percent'] = df['mi_percent'].replace({999: pd.NA}).astype('Int16')\n",
    "        if 'number_of_units' in df.columns:\n",
    "            df['number_of_units'] = df['number_of_units'].replace({99: pd.NA}).astype('Int8')\n",
    "        if 'original_cltv' in df.columns:\n",
    "            df['original_cltv'] = df['original_cltv'].replace({999: pd.NA}).astype('Int16')\n",
    "        if 'original_dti' in df.columns:\n",
    "            df['original_dti'] = df['original_dti'].replace({999: pd.NA}).astype('Int16')\n",
    "        if 'original_ltv' in df.columns:\n",
    "            df['original_ltv'] = df['original_ltv'].replace({999: pd.NA}).astype('Int16')\n",
    "        if 'original_loan_term' in df.columns:\n",
    "            df['original_loan_term'] = df['original_loan_term'].astype('Int16')\n",
    "        if 'number_of_borrowers' in df.columns:\n",
    "            df['number_of_borrowers'] = df['number_of_borrowers'].replace({99: pd.NA}).astype('Int8')\n",
    "        if 'original_upb' in df.columns:\n",
    "            df['original_upb'] = df['original_upb'].astype('Int64')\n",
    "        if 'msa_md' in df.columns:\n",
    "            df['msa_md'] = pd.to_numeric(df['msa_md'], errors='coerce').astype('Int32')\n",
    "\n",
    "        # --- Identifiants / codes ---\n",
    "        if 'loan_sequence_number' in df.columns:\n",
    "            df['loan_sequence_number'] = df['loan_sequence_number'].astype('string')\n",
    "        if 'pre_relief_refi_loan_seq_number' in df.columns:\n",
    "            df['pre_relief_refi_loan_seq_number'] = df['pre_relief_refi_loan_seq_number'].astype('string')\n",
    "\n",
    "        # Postal code : string + padding\n",
    "        if 'postal_code' in df.columns:\n",
    "            s = df['postal_code'].astype('Int64')  # si c'était numérique\n",
    "            s = s.astype('string').str.strip().str.upper()\n",
    "            df['postal_code'] = s.str.zfill(5)\n",
    "\n",
    "        # --- Catégorielles (avec gestion des codes \"NA\") ---\n",
    "        if 'occupancy_status' in df.columns:\n",
    "            s = df['occupancy_status'].astype('string').str.strip().str.upper().replace({'9': pd.NA})\n",
    "            df['occupancy_status'] = s.astype(CategoricalDtype(categories=self.fixed_cats_['occupancy_status'], ordered=False))\n",
    "\n",
    "        if 'channel' in df.columns:\n",
    "            s = df['channel'].astype('string').str.strip().str.upper().replace({'9': pd.NA})\n",
    "            df['channel'] = s.astype(CategoricalDtype(categories=self.fixed_cats_['channel'], ordered=False))\n",
    "\n",
    "        if 'amortization_type' in df.columns:\n",
    "            s = df['amortization_type'].astype('string').str.strip().str.upper()\n",
    "            df['amortization_type'] = s.astype(CategoricalDtype(categories=self.fixed_cats_['amortization_type'], ordered=False))\n",
    "\n",
    "        if 'property_state' in df.columns:\n",
    "            s = df['property_state'].astype('string').str.strip().str.upper()\n",
    "            cats = self.learned_cats_['property_state'] if self.learned_cats_['property_state'] is not None else sorted(s.dropna().unique().tolist())\n",
    "            df['property_state'] = s.astype(CategoricalDtype(categories=cats, ordered=False))\n",
    "\n",
    "        if 'property_type' in df.columns:\n",
    "            s = df['property_type'].astype('string').str.strip().str.upper().replace({'99': pd.NA})\n",
    "            df['property_type'] = s.astype(CategoricalDtype(categories=self.fixed_cats_['property_type'], ordered=False))\n",
    "\n",
    "        if 'loan_purpose' in df.columns:\n",
    "            s = df['loan_purpose'].astype('string').str.strip().str.upper().replace({'9': pd.NA})\n",
    "            df['loan_purpose'] = s.astype(CategoricalDtype(categories=self.fixed_cats_['loan_purpose'], ordered=False))\n",
    "\n",
    "        if 'special_eligibility_program' in df.columns:\n",
    "            s = df['special_eligibility_program'].astype('string').str.strip().str.upper().replace({'9': pd.NA})\n",
    "            df['special_eligibility_program'] = s.astype(CategoricalDtype(categories=self.fixed_cats_['special_eligibility_program'], ordered=False))\n",
    "\n",
    "        if 'property_valuation_method' in df.columns:\n",
    "            # Valeurs 1..4..9 => catégorie (9 => NA)\n",
    "            pvm = pd.to_numeric(df['property_valuation_method'], errors='coerce')\n",
    "            pvm = pvm.replace({9: pd.NA}).astype('Int8')\n",
    "            # Catégories apprises sur le train si disponibles\n",
    "            cats = self.learned_cats_['property_valuation_method']\n",
    "            if cats is None:\n",
    "                cats = sorted(pd.Series(pvm).dropna().unique().tolist())\n",
    "            # On cast en category sur la version stringifiée stable (ex: \"1\",\"2\",...) OU on garde en Int8?\n",
    "            # Ici on garde \"category\" directement depuis les Int8 via astype('category') est ok,\n",
    "            # mais pour figer l'ordre on passe par CategoricalDtype sur la version string.\n",
    "            s = pd.Series(pvm.astype('Int8'), index=df.index)\n",
    "            # Convertir vers string pour unifier les niveaux en catégorie\n",
    "            s_str = s.astype('Int16').astype('string')  # garde <NA>\n",
    "            s_str = s_str.astype(CategoricalDtype(categories=[str(v) for v in cats], ordered=False))\n",
    "            df['property_valuation_method'] = s_str\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "# ---------- Utilisation ----------\n",
    "# 1) Fit sur TRAIN uniquement\n",
    "prep = LoanDataPreprocessor(target_col='default_24m')\n",
    "df_train_prep = prep.fit_transform(df_train)\n",
    "\n",
    "# 2) Transform sur TRAIN et sur VALIDATION\n",
    "df_validation_prep = prep.transform(df_validation)\n",
    "\n",
    "\n",
    "PATH_OUTPUT = \"../data/processed/merged/non_imputed/\"\n",
    "# Save data\n",
    "df_train_prep.to_parquet(f\"{PATH_OUTPUT}train.parquet\")\n",
    "df_validation_prep.to_parquet(f\"{PATH_OUTPUT}validation.parquet\")\n",
    "\n",
    "\n",
    "# Intégration dans un Pipeline scikit-learn (exemple)\n",
    "# pipe = Pipeline([('prep', LoanDataPreprocessor()), ('imp', DataImputer(use_cohort=True)), ('clf', LogisticRegression(...))])\n",
    "# scores = cross_val_score(pipe, X, y, cv=KFold(...))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cda93",
   "metadata": {},
   "source": [
    "# 2) Imputation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive imputation (TODO)\n",
    "# Meilleur code plus bas (dans deux cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Good imputation cell but old\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# # Toggle: imputation par cohorte (année de vintage, etc.)\n",
    "# imput_cohort = False\n",
    "\n",
    "# df = df_train.copy()\n",
    "\n",
    "# # ---------- 0) Drop évident ----------\n",
    "# df.drop(columns=['pre_relief_refi_loan_seq_number'], errors='ignore', inplace=True)\n",
    "\n",
    "# # ---------- Helpers ----------\n",
    "# vyear = df['vintage'].dt.year if 'vintage' in df.columns else None\n",
    "\n",
    "# # ---------- 1) Catés : Unknown / NotApplicable ----------\n",
    "# # NB: first_time_homebuyer_flag est un bool tri-état -> on NE met PAS \"Unknown\" (on garde NA)\n",
    "# # channel : garder Unknown plutôt que mode\n",
    "# if 'channel' in df.columns and isinstance(df['channel'].dtype, CategoricalDtype):\n",
    "#     df['channel'] = df['channel'].cat.add_categories(['Unknown']).fillna('Unknown')\n",
    "\n",
    "# # property_valuation_method : recodage numérique + NotApplicable avant 2017 + NotAvailable\n",
    "# if 'property_valuation_method' in df.columns:\n",
    "#     pvm = pd.to_numeric(df['property_valuation_method'].astype('string'), errors='coerce')\n",
    "#     if vyear is not None:\n",
    "#         # 99 = NotApplicable (avant 2017)\n",
    "#         pvm = pvm.where(vyear >= 2017, 99)\n",
    "#     # 9 = NotAvailable\n",
    "#     pvm = pvm.fillna(9)\n",
    "#     df['property_valuation_method'] = pvm.astype('Int16').astype('category')\n",
    "\n",
    "# # special_eligibility_program : garder la colonne + flag binaire\n",
    "# if 'special_eligibility_program' in df.columns and isinstance(df['special_eligibility_program'].dtype, CategoricalDtype):\n",
    "#     df['special_eligibility_program'] = df['special_eligibility_program'].cat.add_categories(['Unknown']).fillna('Unknown')\n",
    "#     df['has_special_program'] = df['special_eligibility_program'].isin(['H','F','R']).astype('int8')\n",
    "\n",
    "# # msa_md : regrouper NA / non-MSA (0)\n",
    "# if 'msa_md' in df.columns:\n",
    "#     df['msa_md'] = df['msa_md'].fillna(0)\n",
    "\n",
    "# # ---------- 2) CREDIT SCORE : clip + médiane (année × purpose) + flag ----------\n",
    "# if 'credit_score' in df.columns:\n",
    "#     df['cs_missing'] = df['credit_score'].isna().astype('int8')\n",
    "#     cs = pd.to_numeric(df['credit_score'], errors='coerce').clip(lower=300, upper=850)\n",
    "#     if imput_cohort:\n",
    "#         if vyear is not None and 'loan_purpose' in df.columns:\n",
    "#             med = df.groupby([vyear, 'loan_purpose'])['credit_score'].transform('median')\n",
    "#             cs = cs.fillna(med)\n",
    "#         elif 'loan_purpose' in df.columns:\n",
    "#             med = df.groupby(['loan_purpose'])['credit_score'].transform('median')\n",
    "#             cs = cs.fillna(med)\n",
    "#     df['credit_score'] = cs.fillna(cs.median()).round().astype('Int16')\n",
    "\n",
    "# # ---------- 3) MI% : LTV≤80 -> 0 ; sinon médiane (année × LTV bins) + flags ----------\n",
    "# if 'mi_percent' in df.columns:\n",
    "#     df['mi_missing'] = df['mi_percent'].isna().astype('int8')\n",
    "#     mi = pd.to_numeric(df['mi_percent'], errors='coerce').astype('Float32')\n",
    "#     if 'original_ltv' in df.columns:\n",
    "#         ltv = pd.to_numeric(df['original_ltv'], errors='coerce').clip(lower=0)  # sécurité\n",
    "#         # règle métier : si LTV<=80 et MI NaN -> 0\n",
    "#         mi = mi.mask(ltv.le(80) & mi.isna(), 0.0)\n",
    "#         # imputation conditionnelle pour le reste\n",
    "#         ltv_bins = pd.cut(ltv, [0, 80, 90, 95, 100, np.inf], include_lowest=True, right=True)\n",
    "#         if vyear is not None and imput_cohort:\n",
    "#             med = df.groupby([vyear, ltv_bins])['mi_percent'].transform('median')\n",
    "#             mi = mi.fillna(med)\n",
    "#         else:\n",
    "#             med = df.groupby(ltv_bins)['mi_percent'].transform('median')\n",
    "#             mi = mi.fillna(med)\n",
    "#     df['mi_percent'] = mi.fillna(0.0).astype('Float32')\n",
    "#     df['has_mi'] = (df['mi_percent'] > 0).astype('int8')\n",
    "\n",
    "# # ---------- 4) DTI : médiane (année × purpose) + flag ----------\n",
    "# if 'original_dti' in df.columns:\n",
    "#     df['dti_missing'] = df['original_dti'].isna().astype('int8')\n",
    "#     dti = pd.to_numeric(df['original_dti'], errors='coerce')\n",
    "#     if imput_cohort:\n",
    "#         if vyear is not None and 'loan_purpose' in df.columns:\n",
    "#             med = df.groupby([vyear, 'loan_purpose'])['original_dti'].transform('median')\n",
    "#             dti = dti.fillna(med)\n",
    "#         elif 'loan_purpose' in df.columns:\n",
    "#             med = df.groupby(['loan_purpose'])['original_dti'].transform('median')\n",
    "#             dti = dti.fillna(med)\n",
    "#     df['original_dti'] = dti.fillna(dti.median()).round().astype('Int16')\n",
    "\n",
    "# # ---------- 5) CLTV : borne basse = LTV si possible + flag ; puis médiane ----------\n",
    "# if 'original_cltv' in df.columns:\n",
    "#     df['cltv_missing'] = df['original_cltv'].isna().astype('int8')\n",
    "#     cltv = pd.to_numeric(df['original_cltv'], errors='coerce').astype('Float32')\n",
    "#     if 'original_ltv' in df.columns:\n",
    "#         ltv = pd.to_numeric(df['original_ltv'], errors='coerce').astype('Float32')\n",
    "#         # si CLTV NaN & LTV dispo -> CLTV = LTV (borne basse)\n",
    "#         cltv = cltv.fillna(ltv)\n",
    "#         # sécurité : jamais CLTV < LTV\n",
    "#         cltv = np.where(ltv.notna(), np.maximum(cltv, ltv), cltv)\n",
    "#         cltv = pd.Series(cltv, index=df.index).astype('Float32')\n",
    "#     if imput_cohort and vyear is not None:\n",
    "#         med = df.groupby(vyear)['original_cltv'].transform('median')\n",
    "#         cltv = cltv.fillna(med)\n",
    "#     # fallback global si reste des NA\n",
    "#     df['original_cltv'] = pd.Series(cltv, index=df.index).fillna(float(pd.Series(cltv).median())).astype('Float32')\n",
    "\n",
    "# # ---------- 6) Petites ordinales ----------\n",
    "# for col in ['original_loan_term', 'number_of_borrowers']:\n",
    "#     if col in df.columns:\n",
    "#         df[col + '_missing'] = df[col].isna().astype('int8')\n",
    "#         # imputations simples au mode (peu de niveaux, impact limité)\n",
    "#         try:\n",
    "#             df[col] = df[col].fillna(df[col].mode(dropna=True).iloc[0])\n",
    "#         except Exception:\n",
    "#             df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# # ---------- 7) Contrôle ----------\n",
    "# na_left = (df.isna().mean() * 100).round(3)\n",
    "# print(\"NA restants (%):\\n\", na_left[na_left > 0].sort_values(ascending=False))\n",
    "\n",
    "# # Résultat final\n",
    "# df_train_imputed = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac25391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best imputation cell\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, use_cohort=True, missing_flag = False, ltv_bins=(0, 80, 90, 95, 100, np.inf)):\n",
    "        self.use_cohort = use_cohort\n",
    "        self.ltv_bins = ltv_bins\n",
    "        self.missing_flag = missing_flag\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    @staticmethod\n",
    "    def _mode(x):\n",
    "        try:\n",
    "            return x.mode(dropna=True).iloc[0]\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_year(series):\n",
    "        try:\n",
    "            return pd.to_datetime(series).dt.year\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _map_fill(self, keys_tuple_series, mapping):\n",
    "        \"\"\"keys_tuple_series = Series of tuples; mapping = dict {tuple: value}\"\"\"\n",
    "        if mapping is None:\n",
    "            return pd.Series(np.nan, index=keys_tuple_series.index)\n",
    "        return keys_tuple_series.map(mapping)\n",
    "\n",
    "    # ---------- fit on TRAIN ONLY ----------\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy()\n",
    "\n",
    "        # Precompute year if present\n",
    "        vyear = self._to_year(df['vintage']) if 'vintage' in df.columns else None\n",
    "\n",
    "        # STORAGE\n",
    "        self.stats_ = {}\n",
    "\n",
    "        # ---- CREDIT SCORE medians\n",
    "        if 'credit_score' in df.columns:\n",
    "            cs = pd.to_numeric(df['credit_score'], errors='coerce').clip(300, 850)\n",
    "            self.stats_['credit_score_global'] = float(cs.median())\n",
    "            self.stats_['credit_score_by_lp'] = None\n",
    "            self.stats_['credit_score_by_year_lp'] = None\n",
    "\n",
    "            if self.use_cohort and 'loan_purpose' in df.columns:\n",
    "                grp_lp = df[['loan_purpose']].copy()\n",
    "                med_lp = cs.groupby(df['loan_purpose']).median()\n",
    "                self.stats_['credit_score_by_lp'] = med_lp.to_dict()\n",
    "\n",
    "                if vyear is not None:\n",
    "                    keys = list(zip(vyear, df['loan_purpose']))\n",
    "                    med_y_lp = pd.Series(cs.values, index=pd.MultiIndex.from_arrays([vyear, df['loan_purpose']])).groupby(level=[0,1]).median()\n",
    "                    self.stats_['credit_score_by_year_lp'] = {k: float(v) for k,v in med_y_lp.items()}\n",
    "\n",
    "        # ---- MI% medians by LTV bins (and year)\n",
    "        if 'mi_percent' in df.columns and 'original_ltv' in df.columns:\n",
    "            mi = pd.to_numeric(df['mi_percent'], errors='coerce')\n",
    "            ltv = pd.to_numeric(df['original_ltv'], errors='coerce').clip(lower=0)\n",
    "            ltv_bins = pd.cut(ltv, self.ltv_bins, include_lowest=True, right=True)\n",
    "\n",
    "            self.stats_['mi_by_bin'] = mi.groupby(ltv_bins).median().to_dict()\n",
    "            self.stats_['mi_by_year_bin'] = None\n",
    "            if self.use_cohort and vyear is not None :\n",
    "                idx = pd.MultiIndex.from_arrays([vyear, ltv_bins])\n",
    "                med = pd.Series(mi.values, index=idx).groupby(level=[0,1]).median()\n",
    "                self.stats_['mi_by_year_bin'] = {k: float(v) for k,v in med.items()}\n",
    "\n",
    "        # ---- DTI medians\n",
    "        if 'original_dti' in df.columns:\n",
    "            dti = pd.to_numeric(df['original_dti'], errors='coerce')\n",
    "            self.stats_['dti_global'] = float(dti.median())\n",
    "            self.stats_['dti_by_lp'] = None\n",
    "            self.stats_['dti_by_year_lp'] = None\n",
    "\n",
    "            if self.use_cohort and 'loan_purpose' in df.columns:\n",
    "                self.stats_['dti_by_lp'] = dti.groupby(df['loan_purpose']).median().to_dict()\n",
    "                if vyear is not None:\n",
    "                    med = pd.Series(dti.values,\n",
    "                                    index=pd.MultiIndex.from_arrays([vyear, df['loan_purpose']])\n",
    "                                   ).groupby(level=[0,1]).median()\n",
    "                    self.stats_['dti_by_year_lp'] = {k: float(v) for k,v in med.items()}\n",
    "\n",
    "        # ---- CLTV medians by year (fallback global)\n",
    "        if 'original_cltv' in df.columns:\n",
    "            cltv = pd.to_numeric(df['original_cltv'], errors='coerce')\n",
    "            self.stats_['cltv_global'] = float(cltv.median())\n",
    "            self.stats_['cltv_by_year'] = None\n",
    "            if self.use_cohort and vyear is not None:\n",
    "                med = pd.Series(cltv.values, index=vyear).groupby(level=0).median()\n",
    "                self.stats_['cltv_by_year'] = {int(k): float(v) for k,v in med.items() if pd.notna(v)}\n",
    "\n",
    "        # ---- modes for small ordinal\n",
    "        for col in ['original_loan_term', 'number_of_borrowers']:\n",
    "            if col in df.columns:\n",
    "                self.stats_[f'{col}_mode'] = self._mode(df[col])\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ---------- transform (apply to TRAIN and TEST) ----------\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        # 0) Drop\n",
    "        df.drop(columns=['pre_relief_refi_loan_seq_number'], errors='ignore', inplace=True)\n",
    "\n",
    "        if self.missing_flag:\n",
    "            cols_impute = df.columns \n",
    "            missing0 = df[cols_impute].isna().add_prefix('was_missing_').astype('int8')\n",
    "        \n",
    "        \n",
    "\n",
    "        # Helpers\n",
    "        vyear = self._to_year(df['vintage']) if 'vintage' in df.columns else None\n",
    "\n",
    "        # 1) Catés : Unknown / NotApplicable\n",
    "        if 'channel' in df.columns and isinstance(df['channel'].dtype, CategoricalDtype):\n",
    "            df['channel'] = df['channel'].cat.add_categories(['Unknown']).fillna('Unknown')\n",
    "\n",
    "        if 'property_valuation_method' in df.columns:\n",
    "            pvm = pd.to_numeric(df['property_valuation_method'].astype('string'), errors='coerce')\n",
    "            if vyear is not None:\n",
    "                pvm = pvm.where(vyear >= 2017, 99)  # 99 NotApplicable avant 2017\n",
    "            pvm = pvm.fillna(9)  # 9 NotAvailable\n",
    "            df['property_valuation_method'] = pvm.astype('Int16').astype('category')\n",
    "\n",
    "        if 'special_eligibility_program' in df.columns and isinstance(df['special_eligibility_program'].dtype, CategoricalDtype):\n",
    "            df['special_eligibility_program'] = df['special_eligibility_program'].cat.add_categories(['Unknown']).fillna('Unknown')\n",
    "            df['has_special_program'] = df['special_eligibility_program'].isin(['H','F','R']).astype('int8')\n",
    "\n",
    "        if 'msa_md' in df.columns:\n",
    "            df['msa_md'] = df['msa_md'].fillna(0)\n",
    "\n",
    "        # 2) CREDIT SCORE\n",
    "        if 'credit_score' in df.columns:\n",
    "            df['cs_missing'] = df['credit_score'].isna().astype('int8')\n",
    "            cs = pd.to_numeric(df['credit_score'], errors='coerce').clip(300, 850)\n",
    "\n",
    "            # cohort fill\n",
    "            if self.use_cohort and 'loan_purpose' in df.columns:\n",
    "                if vyear is not None and self.stats_.get('credit_score_by_year_lp'):\n",
    "                    keys = pd.Series(list(zip(vyear, df['loan_purpose'])), index=df.index)\n",
    "                    mapped = self._map_fill(keys, self.stats_['credit_score_by_year_lp'])\n",
    "                    cs = cs.fillna(mapped)\n",
    "                if self.stats_.get('credit_score_by_lp'):\n",
    "                    mapped = df['loan_purpose'].map(self.stats_['credit_score_by_lp'])\n",
    "                    cs = cs.fillna(mapped)\n",
    "\n",
    "            # global fallback\n",
    "            cs = cs.fillna(self.stats_.get('credit_score_global', float(np.nan)))\n",
    "            df['credit_score'] = pd.Series(cs, index=df.index).round().astype('Int16')\n",
    "\n",
    "        # 3) MI%\n",
    "        if 'mi_percent' in df.columns:\n",
    "            df['mi_missing'] = df['mi_percent'].isna().astype('int8')\n",
    "            mi = pd.to_numeric(df['mi_percent'], errors='coerce').astype('Float32')\n",
    "\n",
    "            ltv = pd.to_numeric(df['original_ltv'], errors='coerce').clip(lower=0) if 'original_ltv' in df.columns else pd.Series(np.nan, index=df.index)\n",
    "            # règle métier\n",
    "            mi = mi.mask(ltv.le(80) & mi.isna(), 0.0)\n",
    "\n",
    "            # cohort median by LTV bins (and year)\n",
    "            if 'original_ltv' in df.columns:\n",
    "                ltv_bins = pd.cut(ltv, self.ltv_bins, include_lowest=True, right=True)\n",
    "\n",
    "                if self.use_cohort and vyear is not None and self.stats_.get('mi_by_year_bin'):\n",
    "                    keys = pd.Series(list(zip(vyear, ltv_bins)), index=df.index)\n",
    "                    mapped = self._map_fill(keys, self.stats_['mi_by_year_bin'])\n",
    "                    mi = mi.fillna(mapped)\n",
    "\n",
    "                if self.stats_.get('mi_by_bin'):\n",
    "                    mapped = ltv_bins.map(self.stats_['mi_by_bin'])\n",
    "                    mi = mi.fillna(mapped)\n",
    "\n",
    "            df['mi_percent'] = mi.fillna(0.0).astype('Float32')\n",
    "            df['has_mi'] = (df['mi_percent'] > 0).astype('int8')\n",
    "\n",
    "        # 4) DTI\n",
    "        if 'original_dti' in df.columns:\n",
    "            df['dti_missing'] = df['original_dti'].isna().astype('int8')\n",
    "            dti = pd.to_numeric(df['original_dti'], errors='coerce')\n",
    "\n",
    "            if self.use_cohort and 'loan_purpose' in df.columns:\n",
    "                if vyear is not None and self.stats_.get('dti_by_year_lp'):\n",
    "                    keys = pd.Series(list(zip(vyear, df['loan_purpose'])), index=df.index)\n",
    "                    mapped = self._map_fill(keys, self.stats_['dti_by_year_lp'])\n",
    "                    dti = dti.fillna(mapped)\n",
    "                if self.stats_.get('dti_by_lp'):\n",
    "                    mapped = df['loan_purpose'].map(self.stats_['dti_by_lp'])\n",
    "                    dti = dti.fillna(mapped)\n",
    "\n",
    "            dti = dti.fillna(self.stats_.get('dti_global', float(np.nan)))\n",
    "            df['original_dti'] = pd.Series(dti, index=df.index).round().astype('Int16')\n",
    "\n",
    "        # 5) CLTV\n",
    "        if 'original_cltv' in df.columns:\n",
    "            df['cltv_missing'] = df['original_cltv'].isna().astype('int8')\n",
    "            cltv = pd.to_numeric(df['original_cltv'], errors='coerce').astype('Float32')\n",
    "\n",
    "            if 'original_ltv' in df.columns:\n",
    "                ltv = pd.to_numeric(df['original_ltv'], errors='coerce').astype('Float32')\n",
    "                cltv = pd.Series(cltv, index=df.index).fillna(ltv)\n",
    "                cltv = pd.Series(np.where(ltv.notna(), np.maximum(cltv, ltv), cltv), index=df.index).astype('Float32')\n",
    "\n",
    "            if self.use_cohort and vyear is not None and self.stats_.get('cltv_by_year'):\n",
    "                mapped = pd.Series(vyear, index=df.index).map(self.stats_['cltv_by_year'])\n",
    "                cltv = pd.Series(cltv, index=df.index).fillna(mapped)\n",
    "\n",
    "            cltv = pd.Series(cltv, index=df.index).fillna(self.stats_.get('cltv_global', float(np.nan)))\n",
    "            df['original_cltv'] = cltv.astype('Float32')\n",
    "\n",
    "        # 6) Small ordinal → mode\n",
    "        for col in ['original_loan_term', 'number_of_borrowers']:\n",
    "            if col in df.columns:\n",
    "                df[col + '_missing'] = df[col].isna().astype('int8')\n",
    "                mode_val = self.stats_.get(f'{col}_mode', np.nan)\n",
    "                df[col] = df[col].fillna(mode_val)\n",
    "                # si tout était NaN dans le train, sécurité\n",
    "                if df[col].isna().any():\n",
    "                    df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "\n",
    "        # Added missing flag\n",
    "        if self.missing_flag:\n",
    "            df = pd.concat([df, missing0], axis=1)   \n",
    "\n",
    "        return df\n",
    "\n",
    "# ---------- Utilisation ----------\n",
    "# 1) Fit sur TRAIN uniquement\n",
    "imputer = DataImputer(use_cohort=False, missing_flag=False)\n",
    "imputer.fit(df_train)\n",
    "\n",
    "# 2) Transform sur TRAIN et TEST\n",
    "df_train_imp = imputer.transform(df_train_prep)\n",
    "df_validation_imp  = imputer.transform(df_validation_prep)\n",
    "\n",
    "# En cross-val :\n",
    "# pipe = Pipeline([('imp', DataImputer(use_cohort=True)), ('clf', LogisticRegression(...))])\n",
    "# scores = cross_val_score(pipe, X, y, cv=KFold(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Data and the data type after the impute\n",
    "parquet = True\n",
    "PATH_OUTPUT_IMPUTE = \"../data/processed/merged/imputed/\"\n",
    "# If we can use Parquet :\n",
    "if parquet:\n",
    "    df_train_imp.to_parquet(f\"{PATH_OUTPUT_IMPUTE}train.parquet\", index=False)\n",
    "    df_validation_imp.to_parquet(f\"{PATH_OUTPUT_IMPUTE}validation.parquet\", index=False)  \n",
    "\n",
    "\n",
    "\n",
    "# If we want to keep the csv structure :\n",
    "else :\n",
    "    import pickle\n",
    "    from pandas.api.types import CategoricalDtype\n",
    "\n",
    "    dtypes = df_train_imp.dtypes.to_dict()\n",
    "\n",
    "    parse_dates = [c for c, dt in dtypes.items() if str(dt).startswith(\"datetime64\")]\n",
    "    cat_dtypes  = {c: dt for c, dt in dtypes.items() if isinstance(dt, CategoricalDtype)}\n",
    "    other_dtypes = {c: (\"Int64\" if str(dt).startswith(\"int\") and df_train_imp[c].isna().any()\n",
    "                        else dt)\n",
    "                    for c, dt in dtypes.items() if c not in parse_dates and c not in cat_dtypes}\n",
    "\n",
    "\n",
    "    with open(f\"{PATH_OUTPUT_IMPUTE}parse_dates.pkl\",\"wb\") as f:\n",
    "        pickle.dump(parse_dates, f)\n",
    "    with open(f\"{PATH_OUTPUT_IMPUTE}cat_dtypes.pkl\",\"wb\") as f:\n",
    "        pickle.dump(cat_dtypes, f)\n",
    "    with open(f\"{PATH_OUTPUT_IMPUTE}other_dtypes.pkl\",\"wb\") as f:\n",
    "        pickle.dump(other_dtypes, f)\n",
    "\n",
    "    df_train_imp.to_csv(f\"{PATH_OUTPUT_IMPUTE}train_imputed.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    ## IF WE LOAD THE CSV FILE AND WE WANT TO OBTAIN THE EXACT DATA TYPES\n",
    "    # with open(\"../data/processed/notebook/parse_dates.pkl\",\"rb\") as f:\n",
    "    #     loaded_dict = pickle.load(f) \n",
    "    # with open(\"../data/processed/notebook/cat_dtypes.pkl\",\"rb\") as f:\n",
    "    #     cat_dtypes = pickle.load(f)\n",
    "    # with open(\"../data/processed/notebook/other_dtypes.pkl\",\"rb\") as f:\n",
    "    #     other_dtypes = pickle.load(f)\n",
    "\n",
    "\n",
    "    # test = pd.read_csv(\"../data/processed/notebook/train_imputed.csv\",\n",
    "    #                 dtype=other_dtypes, parse_dates=parse_dates)\n",
    "\n",
    "    # for c, dt in cat_dtypes.items():  # réappliquer les catégories exactes\n",
    "    #     test[c] = test[c].astype(dt)\n",
    "\n",
    "    # test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307291ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA PLOT:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eda_plots_by_type(\n",
    "    df: pd.DataFrame,\n",
    "    dict_types: dict,\n",
    "    target: str = 'default_24m',\n",
    "    max_cont: int = 6,\n",
    "    max_cat: int = 6,\n",
    "    top_n_cat: int = 15,\n",
    "    min_share_cat: float = 0.01\n",
    "):\n",
    "    # --- détecte colonnes par type\n",
    "    cont_cols = [c for c,t in dict_types.items() if t == 'continuous' and c in df.columns]\n",
    "    ord_cols  = [c for c,t in dict_types.items() if t == 'integer' and c in df.columns]\n",
    "    cat_cols  = [c for c,t in dict_types.items() if t in ('category','bool') and c in df.columns]\n",
    "    date_cols = [c for c,t in dict_types.items() if t == 'date' and c in df.columns]\n",
    "\n",
    "    # target\n",
    "    y = None\n",
    "    if target in df.columns:\n",
    "        y = df[target]\n",
    "        if pd.api.types.is_bool_dtype(y):\n",
    "            y = y.astype('float32')\n",
    "        else:\n",
    "            y = pd.to_numeric(y, errors='coerce').astype('float32')\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def corr_heatmap(cols, title):\n",
    "        cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if len(cols) < 2: return\n",
    "        cmat = df[cols].corr(method='pearson')\n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        im = plt.imshow(cmat, interpolation='nearest')\n",
    "        plt.title(title)\n",
    "        plt.xticks(np.arange(len(cols)), cols, rotation=60, ha='right')\n",
    "        plt.yticks(np.arange(len(cols)), cols)\n",
    "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_hist(col, bins=50):\n",
    "        s = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "        if s.empty: return\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(s, bins=bins)\n",
    "        plt.title(f\"Distribution: {col}\")\n",
    "        plt.xlabel(col); plt.ylabel(\"Count\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    def plot_continuous_default_rate(col, q=10):\n",
    "        if y is None: return\n",
    "        s = pd.to_numeric(df[col], errors='coerce')\n",
    "        m = s.notna() & y.notna()\n",
    "        s, yt = s[m], y[m]\n",
    "        if s.empty: return\n",
    "        try:\n",
    "            bins = pd.qcut(s, q=q, duplicates='drop')\n",
    "        except ValueError:\n",
    "            bins = pd.cut(s, bins=q, include_lowest=True)\n",
    "        g = pd.DataFrame({'bin': bins, 'y': yt}).groupby('bin').agg(rate=('y','mean'), n=('y','size')).reset_index()\n",
    "        fig = plt.figure(figsize=(7,4))\n",
    "        ax1 = plt.gca(); ax2 = ax1.twinx()\n",
    "        ax1.plot(np.arange(len(g)), g['rate'], marker='o')\n",
    "        ax2.bar(np.arange(len(g)), g['n'], alpha=0.3)\n",
    "        ax1.set_title(f\"Default rate by {col} (binned)\"); ax1.set_xlabel(col+\" (bins)\")\n",
    "        ax1.set_ylabel(\"Default rate\"); ax2.set_ylabel(\"Count\")\n",
    "        ax1.set_xticks(np.arange(len(g))); ax1.set_xticklabels([str(b) for b in g['bin']], rotation=60, ha='right')\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    def plot_categorical_default_rate(col, top_n=15, min_share=0.01):\n",
    "        s = df[col].astype('string')\n",
    "        counts = s.value_counts(dropna=False)\n",
    "        total = counts.sum()\n",
    "        keep = counts[counts/total >= min_share].head(top_n).index\n",
    "        s2 = s.where(s.isin(keep), other='Other')\n",
    "        if y is not None:\n",
    "            m = y.notna()\n",
    "            g = pd.DataFrame({'cat': s2[m], 'y': y[m]}).groupby('cat').agg(rate=('y','mean'), n=('y','size')).reset_index()\n",
    "        else:\n",
    "            g = s2.value_counts(dropna=False).rename_axis('cat').reset_index(name='n')\n",
    "            g['rate'] = np.nan\n",
    "        g = g.sort_values('n', ascending=False)\n",
    "        fig = plt.figure(figsize=(7,4))\n",
    "        ax1 = plt.gca(); ax2 = ax1.twinx()\n",
    "        ax1.bar(g['cat'], g['rate'])\n",
    "        ax2.plot(g['cat'], g['n'], marker='o')\n",
    "        ax1.set_title(f\"Default rate by {col} (top {top_n}, ≥{int(min_share*100)}%)\")\n",
    "        ax1.set_ylabel(\"Default rate\"); ax2.set_ylabel(\"Count\"); ax1.set_xlabel(col)\n",
    "        plt.xticks(rotation=60, ha='right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "    def plot_integer_effect(col):\n",
    "        s = pd.to_numeric(df[col], errors='coerce')\n",
    "        if y is None:\n",
    "            vc = s.value_counts(dropna=False).sort_index()\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.bar(vc.index.astype(str), vc.values)\n",
    "            plt.title(f\"Counts: {col}\"); plt.xlabel(col); plt.ylabel(\"Count\")\n",
    "            plt.tight_layout(); plt.show(); return\n",
    "        m = s.notna() & y.notna(); s, yt = s[m], y[m]\n",
    "        nun = s.nunique()\n",
    "        if nun <= 15:\n",
    "            g = pd.DataFrame({'x': s, 'y': yt}).groupby('x').agg(rate=('y','mean'), n=('y','size')).reset_index()\n",
    "            fig = plt.figure(figsize=(6,4))\n",
    "            ax1 = plt.gca(); ax2 = ax1.twinx()\n",
    "            ax1.bar(g['x'].astype(str), g['rate'])\n",
    "            ax2.plot(g['x'].astype(str), g['n'], marker='o')\n",
    "            ax1.set_title(f\"Default rate by {col}\")\n",
    "            ax1.set_ylabel(\"Default rate\"); ax2.set_ylabel(\"Count\"); ax1.set_xlabel(col)\n",
    "            plt.tight_layout(); plt.show()\n",
    "        else:\n",
    "            plot_continuous_default_rate(col, q=min(10, nun))\n",
    "\n",
    "    def plot_time_series(col):\n",
    "        s = df[col]\n",
    "        if isinstance(s.dtype, pd.PeriodDtype):\n",
    "            ts = s.dt.to_timestamp()\n",
    "        else:\n",
    "            ts = pd.to_datetime(s, errors='coerce')\n",
    "        if y is None:\n",
    "            g = ts.value_counts().sort_index()\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.plot(g.index, g.values)\n",
    "            plt.title(f\"Counts over time: {col}\"); plt.xlabel(\"Date\"); plt.ylabel(\"Count\")\n",
    "            plt.tight_layout(); plt.show(); return\n",
    "        data = pd.DataFrame({'t': ts, 'y': y}).dropna()\n",
    "        if data.empty: return\n",
    "        data['t'] = data['t'].dt.to_period('M').dt.to_timestamp()\n",
    "        g = data.groupby('t').agg(rate=('y','mean'), n=('y','size')).reset_index()\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        ax1 = plt.gca(); ax2 = ax1.twinx()\n",
    "        ax1.plot(g['t'], g['rate'], marker='o'); ax2.bar(g['t'], g['n'], alpha=0.3)\n",
    "        ax1.set_title(f\"Default rate over time: {col}\")\n",
    "        ax1.set_xlabel(\"Date\"); ax1.set_ylabel(\"Default rate\"); ax2.set_ylabel(\"Count\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ---------- plots ----------\n",
    "    if len(cont_cols) >= 2:\n",
    "        corr_heatmap(cont_cols, \"Pearson correlation (continuous vars)\")\n",
    "\n",
    "    for c in cont_cols[:max_cont]:\n",
    "        plot_hist(c, bins=50)\n",
    "        plot_continuous_default_rate(c, q=10)\n",
    "\n",
    "    preferred_cats = ['occupancy_status','channel','property_type','loan_purpose','msa_md','property_valuation_method']\n",
    "    cat_to_plot = [c for c in preferred_cats if c in cat_cols][:max_cat]\n",
    "    if not cat_to_plot:\n",
    "        cat_to_plot = cat_cols[:max_cat]\n",
    "    for c in cat_to_plot:\n",
    "        plot_categorical_default_rate(c, top_n=top_n_cat, min_share=min_share_cat)\n",
    "\n",
    "    for c in ord_cols[:max(1, min(2, len(ord_cols)))]:\n",
    "        plot_integer_effect(c)\n",
    "\n",
    "    for c in date_cols[:2]:\n",
    "        plot_time_series(c)\n",
    "\n",
    "# --- Utilisation ---\n",
    "eda_plots_by_type(df_train, dict_types,\n",
    "                  target='default_24m',\n",
    "                  max_cont=6, max_cat=6, top_n_cat=15, min_share_cat=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1666e5a4",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156c394",
   "metadata": {},
   "source": [
    "## Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_parquet(\"../data/processed/default_labels/\")\n",
    "df_train_imp = pd.read_parquet(\"../data/processed/default_labels_imputed/train.parquet\")\n",
    "df_validation_imp = pd.read_parquet(\"../data/processed/default_labels_imputed/validation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e159595",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"first_time_homebuyer_flag\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MAP each column into a data types categories (to better select columns)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dict_types = {\n",
    " 'credit_score': 'continuous',\n",
    " 'first_payment_date': 'date',\n",
    " 'first_time_homebuyer_flag': 'bool',            # Y/N, 9 -> NA\n",
    " 'maturity_date': 'date',\n",
    " 'msa_md': 'category',\n",
    " 'mi_percent': 'continuous',                     # pas catégoriel\n",
    " 'number_of_units': 'category',                  # 1–4, 99 -> NA (caté OK)\n",
    " 'occupancy_status': 'category',\n",
    " 'original_cltv': 'continuous',\n",
    " 'original_dti': 'continuous',\n",
    " 'original_upb': 'continuous',\n",
    " 'original_ltv': 'continuous',\n",
    " 'original_interest_rate': 'continuous',\n",
    " 'channel': 'category',\n",
    " 'ppm_flag': 'bool',                             # Y/N\n",
    " 'amortization_type': 'category',                # FRM/ARM\n",
    " 'property_state': 'category',\n",
    " 'property_type': 'category',\n",
    " 'postal_code': 'category',\n",
    " 'loan_sequence_number': 'id',\n",
    " 'loan_purpose': 'category',\n",
    " 'original_loan_term': 'integer',                # nb de mois, pas une caté\n",
    " 'number_of_borrowers': 'integer',               # petit entier\n",
    " 'seller_name': 'category',\n",
    " 'servicer_name': 'category',\n",
    " 'super_conforming_flag': 'bool',                # Y / vide\n",
    " 'pre_relief_refi_loan_seq_number': 'id_ref',    # clé de ref, pas une caté\n",
    " 'special_eligibility_program': 'category',      # H/F/R/9\n",
    " 'relief_refinance_indicator': 'bool',           # Y / vide\n",
    " 'property_valuation_method': 'category',        # codes 1–4, 9 -> NA\n",
    " 'interest_only_indicator': 'bool',              # Y/N\n",
    " 'mi_cancellation_indicator': 'category',        # Y/N/7/9\n",
    " 'default_24m': 'bool',\n",
    " 'vintage': 'date'                               # ex. YYYY-Q à partir de FIRST_PAYMENT_DATE\n",
    "}\n",
    "\n",
    "# inversion type -> liste de colonnes\n",
    "_dict_types_col = defaultdict(list)\n",
    "for col, t in dict_types.items():\n",
    "    _dict_types_col[t].append(col)\n",
    "\n",
    "dict_types_col = dict(_dict_types_col)  # (optionnel) convertir en dict standard\n",
    "\n",
    "cont_cols  = dict_types_col.get('continuous', [])\n",
    "ord_cols   = dict_types_col.get('integer', [])      # ici: original_loan_term, number_of_borrowers (traités comme ordinal/binaire)\n",
    "bools_cols = dict_types_col.get('bool', [])\n",
    "cat_cols  = dict_types_col.get('category', []) + bools_cols\n",
    "\n",
    "\n",
    "\n",
    "# Correlations cell\n",
    "\n",
    "from scipy.stats import spearmanr, pointbiserialr, pearsonr, kendalltau\n",
    "from scipy.stats import chi2_contingency\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "spearman_pairs  = {}\n",
    "pearson_pairs   = {}\n",
    "cramer_pairs    = {}\n",
    "pb_pairs        = {}\n",
    "kendall_pairs   = {}\n",
    "\n",
    "# 1) Spearman : ordinal <-> continu (monotone)\n",
    "for x in ord_cols:\n",
    "    for y in cont_cols:\n",
    "        s = df_train_imp[[x, y]].dropna()\n",
    "        if s.empty: continue\n",
    "        r, p = spearmanr(s[x], s[y])\n",
    "        spearman_pairs[(x, y)] = (r, p)\n",
    "\n",
    "# 2) Pearson : continu <-> continu (sans doublons ni diagonale)\n",
    "for x, y in combinations(cont_cols, 2):\n",
    "    s = df_train_imp[[x, y]].dropna()\n",
    "    if s.empty: continue\n",
    "    r, p = pearsonr(s[x], s[y])\n",
    "    pearson_pairs[(x, y)] = (r, p)\n",
    "\n",
    "# 3) Point-bisérial : continu <-> booléen binaire\n",
    "bin_cols = [c for c in dict_types_col.get('bool', [])\n",
    "            if set(df_train_imp[c].dropna().unique()).issubset({0,1,True,False})]\n",
    "for x in cont_cols:\n",
    "    for b in bin_cols:\n",
    "        s = df_train_imp[[x, b]].dropna()\n",
    "        if s.empty: continue\n",
    "        r, p = pointbiserialr(s[x], s[b].astype(int))\n",
    "        pb_pairs[(x, b)] = (r, p)   # <-- bug corrigé\n",
    "\n",
    "# 4) Kendall tau-b : ordinal <-> ordinal (et en option ordinal <-> continu)\n",
    "for x, y in combinations(ord_cols, 2):\n",
    "    s = df_train_imp[[x, y]].dropna()\n",
    "    if s.empty: continue\n",
    "    tau, p = kendalltau(s[x], s[y], method=\"auto\")  # tau-b avec correction des ties\n",
    "    kendall_pairs[(x, y)] = (tau, p)\n",
    "\n",
    "# Optionnel : Kendall pour ordinal <-> continu\n",
    "for x in ord_cols:\n",
    "    for y in cont_cols:\n",
    "        s = df_train_imp[[x, y]].dropna()\n",
    "        if s.empty: continue\n",
    "        tau, p = kendalltau(s[x], s[y], method=\"auto\")\n",
    "        kendall_pairs[(x, y)] = (tau, p)\n",
    "\n",
    "# 5) Cramér V corrigé : nominal/ordinal <-> nominal/ordinal\n",
    "def cramers_v_corrected(x, y):\n",
    "    tbl = pd.crosstab(x, y)\n",
    "    if tbl.size == 0: \n",
    "        return np.nan\n",
    "    chi2 = chi2_contingency(tbl, correction=False)[0]\n",
    "    n = tbl.values.sum()\n",
    "    r, k = tbl.shape\n",
    "    phi2 = chi2 / n\n",
    "    # Correction biais (Bergsma, 2013)\n",
    "    phi2corr = max(0, phi2 - (k-1)*(r-1)/(n-1))\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    denom = max(1e-12, min((kcorr-1), (rcorr-1)))\n",
    "    return np.sqrt(phi2corr / denom)\n",
    "\n",
    "cats = list(set(cat_cols) | set(ord_cols))  # traiter l'ordinal comme catégoriel ici\n",
    "for a, b in combinations(cats, 2):\n",
    "    s = df_train_imp[[a, b]].dropna()\n",
    "    if s.empty: continue\n",
    "    cramer_pairs[(a, b)] = cramers_v_corrected(s[a], s[b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape the correlation into a long data frame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- 1) Fusion des dictionnaires en un tableau \"long\" ----------\n",
    "def _dict_to_df(d, measure, has_p=True):\n",
    "    rows = []\n",
    "    for (x, y), val in d.items():\n",
    "        # tolère (stat, p) ou (stat, p, n)\n",
    "        if has_p:\n",
    "            stat = val[0]\n",
    "            pval = val[1]\n",
    "            n = val[2] if (isinstance(val, (tuple, list)) and len(val) >= 3) else np.nan\n",
    "        else:\n",
    "            stat = val\n",
    "            pval = np.nan\n",
    "            n = np.nan\n",
    "        rows.append({\n",
    "            \"var_x\": x,\n",
    "            \"var_y\": y,\n",
    "            \"measure\": measure,\n",
    "            \"stat\": float(stat),\n",
    "            \"p_value\": float(pval) if pd.notna(pval) else np.nan,\n",
    "            \"n\": float(n) if pd.notna(n) else np.nan,\n",
    "            \"abs_stat\": abs(float(stat)),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "dfs = []\n",
    "if len(spearman_pairs): dfs.append(_dict_to_df(spearman_pairs, \"spearman\", has_p=True))\n",
    "if len(pearson_pairs):  dfs.append(_dict_to_df(pearson_pairs,  \"pearson\",  has_p=True))\n",
    "if len(pb_pairs):       dfs.append(_dict_to_df(pb_pairs,       \"pointbiserial\", has_p=True))\n",
    "if len(kendall_pairs):  dfs.append(_dict_to_df(kendall_pairs,  \"kendall_tau\", has_p=True))\n",
    "if len(cramer_pairs):   dfs.append(_dict_to_df(cramer_pairs,   \"cramers_v\", has_p=False))\n",
    "\n",
    "assocs_long = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame(\n",
    "    columns=[\"var_x\",\"var_y\",\"measure\",\"stat\",\"p_value\",\"n\",\"abs_stat\"]\n",
    ")\n",
    "\n",
    "# ---------- 2) Ajout des types de variables (utile pour filtrer) ----------\n",
    "def _type_of(v):\n",
    "    if v in bin_cols:  return \"bin\"\n",
    "    if v in cont_cols: return \"cont\"\n",
    "    if v in ord_cols:  return \"ord\"\n",
    "    if v in cat_cols:  return \"cat\"\n",
    "    return \"unknown\"\n",
    "\n",
    "assocs_long[\"type_x\"] = assocs_long[\"var_x\"].map(_type_of)\n",
    "assocs_long[\"type_y\"] = assocs_long[\"var_y\"].map(_type_of)\n",
    "\n",
    "# ---------- 3) Tri + étoiles de significativité + q-values (FDR BH) ----------\n",
    "def _stars(p):\n",
    "    if pd.isna(p): return \"\"\n",
    "    return \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "\n",
    "assocs_long[\"sig\"] = assocs_long[\"p_value\"].apply(_stars)\n",
    "\n",
    "# FDR Benjamini–Hochberg (si statsmodels dispo)\n",
    "try:\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    m = assocs_long[\"p_value\"].notna()\n",
    "    assocs_long.loc[m, \"q_value\"] = multipletests(assocs_long.loc[m, \"p_value\"], method=\"fdr_bh\")[1]\n",
    "except Exception:\n",
    "    assocs_long[\"q_value\"] = np.nan\n",
    "\n",
    "assocs_long = assocs_long.sort_values([\"measure\", \"abs_stat\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# ---------- 4) Exemples d’usages rapides ----------\n",
    "# a) Top 20 associations par mesure (en valeur absolue)\n",
    "top20_by_measure = (\n",
    "    assocs_long.groupby(\"measure\", group_keys=False)\n",
    "               .apply(lambda g: g.nlargest(20, \"abs_stat\"))\n",
    "               .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# b) Pivots pour heatmaps (ex: Pearson et Cramér V)\n",
    "pearson_pivot = (assocs_long.query(\"measure == 'pearson'\")\n",
    "                              .pivot(index=\"var_x\", columns=\"var_y\", values=\"stat\"))\n",
    "cramers_pivot = (assocs_long.query(\"measure == 'cramers_v'\")\n",
    "                              .pivot(index=\"var_x\", columns=\"var_y\", values=\"stat\"))\n",
    "\n",
    "# ---------- 5) Export CSV ----------\n",
    "assocs_long.to_csv(\"associations_long.csv\", index=False)\n",
    "top20_by_measure.to_csv(\"associations_top20_by_measure.csv\", index=False)\n",
    "\n",
    "print(\"OK • assocs_long :\", assocs_long.shape, \"| Sauvé: associations_long.csv\")\n",
    "assocs_long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(assocs_long[\"measure\"])\n",
    "dict_test = {'cramers_v':0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformation cell (drop, transformation..., allow for multiple transformation)\n",
    "## Naive transformation / Transformation for linear model / For non linear model...\n",
    "## Regularisat\n",
    "# ion or not...\n",
    "\n",
    "# pre treatment\n",
    "# df_train_clean[bools_cols].sum()\n",
    "\n",
    "# We then drop modality with no variability\n",
    "# df_train_clean.drop(columns=[\"relief_refinance_indicator\",\"interest_only_indicator\",\"ppm_flag\"], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895097e",
   "metadata": {},
   "source": [
    "# Graph, data viz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_train[\"default_24m\"].value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "# 4) Tracé Matplotlib (et non seaborn)\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(counts.index.astype(str), counts.values, color=\"lightblue\")\n",
    "\n",
    "ax.set_title(\"Default modality proportion\")\n",
    "ax.set_xlabel(\"Modalité (default_24m)\")\n",
    "ax.set_ylabel(\"Pourcentage (%)\")\n",
    "\n",
    "# Légendes de pourcentage sur chaque barre\n",
    "ax.bar_label(bars, labels=[f\"{v:.1f}%\" for v in counts.values], padding=3)\n",
    "\n",
    "# Marges et rendu propre\n",
    "ax.set_ylim(0, max(100, counts.max() * 1.15))   # laisse un peu d'espace pour le label\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c91a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Données (sans NaN)\n",
    "x = df_train[\"original_upb\"].dropna().values\n",
    "\n",
    "# Bins robustes (Freedman–Diaconis) ; repli sur 'auto' si nécessaire\n",
    "try:\n",
    "    bins = np.histogram_bin_edges(x, bins='fd')\n",
    "    if np.isinf(bins).any() or len(bins) < 2:\n",
    "        raise ValueError\n",
    "except Exception:\n",
    "    bins = 'auto'\n",
    "\n",
    "# Mise en forme \"milliers avec espace\"\n",
    "fmt = FuncFormatter(lambda v, pos: f\"{v:,.0f}\".replace(\",\", \" \"))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5), dpi=120)\n",
    "\n",
    "# Histogramme (effectifs)\n",
    "n, b, patches = ax.hist(\n",
    "    x, bins=bins, density=False,\n",
    "    edgecolor='white', linewidth=0.8, alpha=0.9\n",
    ")\n",
    "\n",
    "# Lignes moyenne / médiane\n",
    "moy = np.mean(x)\n",
    "med = np.median(x)\n",
    "ax.axvline(moy, linestyle='--', linewidth=1.6, label=f\"Moyenne = {moy:,.0f}\".replace(\",\", \" \"))\n",
    "ax.axvline(med, linestyle=':',  linewidth=1.6, label=f\"Médiane = {med:,.0f}\".replace(\",\", \" \"))\n",
    "\n",
    "# Habillage\n",
    "ax.set_title(\"Distribution de original_upb\", pad=10)\n",
    "ax.set_xlabel(\"original_upb\")\n",
    "ax.set_ylabel(\"Effectifs\")\n",
    "ax.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "ax.xaxis.set_major_formatter(fmt)\n",
    "ax.yaxis.set_major_formatter(fmt)\n",
    "ax.legend(frameon=False, loc='upper right')\n",
    "\n",
    "# (Optionnel) Utile si très asymétrique :\n",
    "# ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : df est votre DataFrame et la colonne s’appelle 'status'\n",
    "# 1) Lister les modalités (valeurs distinctes)\n",
    "df_train['default_24m'].unique()\n",
    "\n",
    "# 2) Compter chaque modalité\n",
    "df['status'].value_counts()\n",
    "\n",
    "# 3) N’afficher que les lignes dont la modalité est 'default'\n",
    "df_default = df[df['status'].eq('default')]        # équiv. : df.query(\"status == 'default'\")\n",
    "\n",
    "# 4) Nombre et proportion de 'default'\n",
    "nb_default = df['status'].eq('default').sum()\n",
    "prop_default = df['status'].eq('default').mean()   # proportion entre 0 et 1\n",
    "\n",
    "# 5) Si la colonne est codée 0/1, la mapper vers des libellés puis (optionnel) en catégorie ordonnée\n",
    "df['status'] = df['status'].map({1: 'default', 0: 'non-default'}).astype('category')\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "cat = CategoricalDtype(categories=['non-default', 'default'], ordered=True)\n",
    "df['status'] = df['status'].astype(cat)\n",
    "\n",
    "# 6) Afficher la catégorie 'default' (si dtype catégoriel) et vérifier qu’elle existe\n",
    "'default' in df['status'].cat.categories\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd-calibration-NMoSp0fs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
