{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f229fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing flags (train): ['cs_missing', 'mi_missing', 'dti_missing', 'cltv_missing', 'original_loan_term_missing', 'number_of_borrowers_missing']\n",
      "(9590892, 35)\n",
      "(931745, 35)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 901\u001b[39m\n\u001b[32m    894\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m — \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Gini = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, nb_points=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_cum)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    896\u001b[39m \u001b[38;5;66;03m# =======================\u001b[39;00m\n\u001b[32m    897\u001b[39m \u001b[38;5;66;03m# Exemple d'utilisation :\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[38;5;66;03m# =======================\u001b[39;00m\n\u001b[32m    899\u001b[39m \n\u001b[32m    900\u001b[39m \u001b[38;5;66;03m# 1) Fit sur train (DF \"one-hot\" ou mélange)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m res = \u001b[43mrun_full_pipeline_on_onehot_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_onehot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_train_imp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault_24m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 👈 sera PROTÉGÉ du dé-one-hot\u001b[39;49;00m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_bins_categ\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_bin_size_categ\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_bins_num\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mmin_bin_size_num\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_quantiles_num\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_label\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__MISSING__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_col_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__BIN\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs_categ\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs_num\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ← utilise tous les cœurs disponibles\u001b[39;49;00m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# min_gini_keep=1e-6,        # ← décommente pour exclure auto les variables trop faibles\u001b[39;49;00m\n\u001b[32m    910\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[38;5;66;03m# 2) Jeu final pour le modèle (X, y)\u001b[39;00m\n\u001b[32m    913\u001b[39m cols_id = [\u001b[33m\"\u001b[39m\u001b[33mloan_sequence_number\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpostal_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mseller_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mservicer_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmsa_md\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 733\u001b[39m, in \u001b[36mrun_full_pipeline_on_onehot_df\u001b[39m\u001b[34m(df_onehot, target_col, max_bins_categ, min_bin_size_categ, max_bins_num, min_bin_size_num, n_quantiles_num, include_missing, missing_label, max_levels_object, bin_col_suffix, exclude_ids, n_jobs_categ, n_jobs_num, verbose, min_gini_keep)\u001b[39m\n\u001b[32m    730\u001b[39m cat_cols = find_categorical_columns(DF, target_col=TARGET, max_levels_object=max_levels_object,\n\u001b[32m    731\u001b[39m                                     exclude_ids=exclude_ids)\n\u001b[32m    732\u001b[39m ordinal_cols, explicit_orders = extract_ordinal_info(DF, cat_cols)\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m out_cat = \u001b[43mauto_bin_all_categoricals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcat_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTARGET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_missing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mordinal_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mordinal_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicit_orders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexplicit_orders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_bins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_bins_categ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_bin_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_bin_size_categ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder_key_for_curve\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbad_rate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnominal_order_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbad_rate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_binned_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_col_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbin_col_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs_categ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[38;5;66;03m# 3) numériques\u001b[39;00m\n\u001b[32m    744\u001b[39m out_num = auto_bin_all_numerics(\n\u001b[32m    745\u001b[39m     df=out_cat[\u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m], target_col=TARGET,\n\u001b[32m    746\u001b[39m     max_bins=max_bins_num, min_bin_size=min_bin_size_num,\n\u001b[32m   (...)\u001b[39m\u001b[32m    750\u001b[39m     n_jobs=n_jobs_num, verbose=verbose\n\u001b[32m    751\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 542\u001b[39m, in \u001b[36mauto_bin_all_categoricals\u001b[39m\u001b[34m(df, cat_columns, target_col, include_missing, missing_label, ordinal_cols, explicit_orders, max_bins, min_bin_size, order_key_for_curve, nominal_order_key, add_binned_columns, bin_col_suffix, n_jobs, verbose)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_jobs != \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m Parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cat_columns) > \u001b[32m0\u001b[39m:\n\u001b[32m    531\u001b[39m     tasks = (\n\u001b[32m    532\u001b[39m         delayed(_compute_cat_bin_result)(\n\u001b[32m    533\u001b[39m             df_out[[col, target_col]].copy(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    540\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cat_columns\n\u001b[32m    541\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     out = Parallel(n_jobs=n_jobs, backend=\u001b[33m\"\u001b[39m\u001b[33mloky\u001b[39m\u001b[33m\"\u001b[39m, verbose=verbose)(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    543\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col, res \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[32m    544\u001b[39m         results[col] = res\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 533\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# 1) calcule les binnings en parallèle (pas de mapping ici)\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_jobs != \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m Parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cat_columns) > \u001b[32m0\u001b[39m:\n\u001b[32m    531\u001b[39m     tasks = (\n\u001b[32m    532\u001b[39m         delayed(_compute_cat_bin_result)(\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m             \u001b[43mdf_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.copy(),\n\u001b[32m    534\u001b[39m             col, target_col,\n\u001b[32m    535\u001b[39m             include_missing, missing_label,\n\u001b[32m    536\u001b[39m             (col \u001b[38;5;129;01min\u001b[39;00m ordinal_cols), explicit_orders.get(col),\n\u001b[32m    537\u001b[39m             max_bins, min_bin_size,\n\u001b[32m    538\u001b[39m             order_key_for_curve, nominal_order_key\n\u001b[32m    539\u001b[39m         )\n\u001b[32m    540\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cat_columns\n\u001b[32m    541\u001b[39m     )\n\u001b[32m    542\u001b[39m     out = Parallel(n_jobs=n_jobs, backend=\u001b[33m\"\u001b[39m\u001b[33mloky\u001b[39m\u001b[33m\"\u001b[39m, verbose=verbose)(\u001b[38;5;28mlist\u001b[39m(tasks))\n\u001b[32m    543\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col, res \u001b[38;5;129;01min\u001b[39;00m out:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/pd-calibration-NMoSp0fs-py3.12/lib/python3.12/site-packages/pandas/core/frame.py:4122\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   4120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._slice(indexer, axis=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4122\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   4125\u001b[39m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[32m   4126\u001b[39m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[32m   4127\u001b[39m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[32m   4128\u001b[39m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[32m   4129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n\u001b[32m   4130\u001b[39m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/pd-calibration-NMoSp0fs-py3.12/lib/python3.12/site-packages/pandas/core/generic.py:4172\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4161\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4163\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4164\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4165\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4170\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4171\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4172\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4173\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/pd-calibration-NMoSp0fs-py3.12/lib/python3.12/site-packages/pandas/core/generic.py:4152\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4147\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4148\u001b[39m     indices = np.arange(\n\u001b[32m   4149\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4150\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4152\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4154\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4158\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4159\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/pd-calibration-NMoSp0fs-py3.12/lib/python3.12/site-packages/pandas/core/internals/managers.py:894\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    891\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    893\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/pd-calibration-NMoSp0fs-py3.12/lib/python3.12/site-packages/pandas/core/internals/managers.py:680\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequested axis not found in manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     new_blocks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    687\u001b[39m     new_blocks = [\n\u001b[32m    688\u001b[39m         blk.take_nd(\n\u001b[32m    689\u001b[39m             indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    696\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/pd-calibration-NMoSp0fs-py3.12/lib/python3.12/site-packages/pandas/core/internals/managers.py:818\u001b[39m, in \u001b[36mBaseBlockManager._slice_take_blocks_ax0\u001b[39m\u001b[34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[39m\n\u001b[32m    816\u001b[39m deep = \u001b[38;5;129;01mnot\u001b[39;00m (only_slice \u001b[38;5;129;01mor\u001b[39;00m using_copy_on_write())\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mgr_loc \u001b[38;5;129;01min\u001b[39;00m mgr_locs:\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     newblk = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    819\u001b[39m     newblk.mgr_locs = BlockPlacement(\u001b[38;5;28mslice\u001b[39m(mgr_loc, mgr_loc + \u001b[32m1\u001b[39m))\n\u001b[32m    820\u001b[39m     blocks.append(newblk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/pd-calibration-NMoSp0fs-py3.12/lib/python3.12/site-packages/pandas/core/internals/blocks.py:822\u001b[39m, in \u001b[36mBlock.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    820\u001b[39m refs: BlockValuesRefs | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     values = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m     refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/pd-calibration-NMoSp0fs-py3.12/lib/python3.12/site-packages/pandas/core/arrays/masked.py:1006\u001b[39m, in \u001b[36mBaseMaskedArray.copy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Self:\n\u001b[32m   1005\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._data.copy()\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._simple_new(data, mask)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "no_flag_missing = True\n",
    "\n",
    "## Load Imputed data\n",
    "df_train_imp = pd.read_parquet(\"../data/processed/merged/imputed/train.parquet\")\n",
    "df_train_imp['first_time_homebuyer_flag'] = df_train_imp['first_time_homebuyer_flag'].fillna(False) # add this line into the imputer\n",
    "\n",
    "df_val_imp = pd.read_parquet(\"../data/processed/merged/imputed/validation.parquet\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if no_flag_missing :\n",
    "    # 1) Lister les colonnes \"missing\" (suffixe _missing, insensible à la casse)\n",
    "    missing_flag_cols = [c for c in df_train_imp.columns if re.search(r'(?i)_missing$', c)]\n",
    "    print(\"Missing flags (train):\", missing_flag_cols)\n",
    "\n",
    "    # 2) Créer des vues SANS ces flags (train/val)\n",
    "    df_train_imp = df_train_imp.drop(columns=missing_flag_cols, errors=\"ignore\")\n",
    "    df_val_imp   = df_val_imp.drop(columns=missing_flag_cols, errors=\"ignore\")  # si tu veux traiter la val ensuite\n",
    "\n",
    "\n",
    "print(df_train_imp.shape)\n",
    "print(df_val_imp.shape)\n",
    "\n",
    "\n",
    "# Méhode 1 opti :\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Pipeline \"max |Gini|\" — version complète, robuste et parallélisée\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parallélisation\n",
    "try:\n",
    "    from joblib import Parallel, delayed\n",
    "except Exception:  # fallback si joblib indisponible\n",
    "    Parallel = None\n",
    "    def delayed(f): return f\n",
    "\n",
    "# --------------------------------------------\n",
    "# Chargement des données (imputed uniquement)\n",
    "# --------------------------------------------\n",
    "df_train_imp = pd.read_parquet(\"../data/processed/merged/imputed/train.parquet\")\n",
    "df_val_imp   = pd.read_parquet(\"../data/processed/merged/imputed/validation.parquet\")\n",
    "\n",
    "# Ajustement spécifique (idéalement à déplacer dans l'imputer)\n",
    "if \"first_time_homebuyer_flag\" in df_train_imp.columns:\n",
    "    df_train_imp[\"first_time_homebuyer_flag\"] = df_train_imp[\"first_time_homebuyer_flag\"].fillna(False)\n",
    "if \"first_time_homebuyer_flag\" in df_val_imp.columns:\n",
    "    df_val_imp[\"first_time_homebuyer_flag\"] = df_val_imp[\"first_time_homebuyer_flag\"].fillna(False)\n",
    "\n",
    "# ============================================\n",
    "# Utils Gini (X=Good, Y=Bad)\n",
    "# ============================================\n",
    "def gini_trapz(df_cum,\n",
    "               y_col=\"bad_client_share_cumsum\",\n",
    "               x_col=\"good_client_share_cumsum\",\n",
    "               signed=False):\n",
    "    \"\"\"\n",
    "    Gini = 1 - 2 * aire(y vs x).\n",
    "    Par défaut on renvoie |Gini| (signed=False).\n",
    "    Sécurise les endpoints (0,0) et (1,1) et clamp dans [0,1].\n",
    "    \"\"\"\n",
    "    df = df_cum[[x_col, y_col]].astype(float).copy().sort_values(x_col)\n",
    "    # clamp\n",
    "    df[x_col] = df[x_col].clip(0, 1)\n",
    "    df[y_col] = df[y_col].clip(0, 1)\n",
    "    # endpoints\n",
    "    if df[x_col].iloc[0] > 0 or df[y_col].iloc[0] > 0:\n",
    "        df = pd.concat([pd.DataFrame({x_col: [0.0], y_col: [0.0]}), df], ignore_index=True)\n",
    "    if df[x_col].iloc[-1] < 1 - 1e-12 or df[y_col].iloc[-1] < 1 - 1e-12:\n",
    "        df = pd.concat([df, pd.DataFrame({x_col: [1.0], y_col: [1.0]})], ignore_index=True)\n",
    "    x = df[x_col].to_numpy()\n",
    "    y = df[y_col].to_numpy()\n",
    "    area = np.trapezoid(y, x) if hasattr(np, \"trapezoid\") else np.trapz(y, x)\n",
    "    g = 1 - 2 * area\n",
    "    return g if signed else abs(g)\n",
    "\n",
    "# ======================================================\n",
    "# Étape 0 — Dé-one-hot (protège la cible)\n",
    "# ======================================================\n",
    "def detect_onehot_groups(df, allow_singleton=True, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Détecte les groupes one-hot en scindant au DERNIER underscore.\n",
    "    Accepte tout suffixe (state_CA, grade_A, ...). 0/1 ou bool requis.\n",
    "    exclude_cols : colonnes à ignorer (ex: la cible).\n",
    "    \"\"\"\n",
    "    exclude = set(exclude_cols or [])\n",
    "    groups = {}\n",
    "    for col in df.columns:\n",
    "        if col in exclude or \"_\" not in col:\n",
    "            continue\n",
    "        base, label = col.rsplit(\"_\", 1)\n",
    "        s = df[col]\n",
    "        is_ohe = (pd.api.types.is_bool_dtype(s) or\n",
    "                  (pd.api.types.is_numeric_dtype(s) and s.dropna().isin([0, 1]).all()))\n",
    "        if is_ohe:\n",
    "            groups.setdefault(base, []).append((col, label))\n",
    "\n",
    "    clean = {}\n",
    "    for base, items in groups.items():\n",
    "        if len(items) >= 2 or allow_singleton:\n",
    "            clean[base] = items\n",
    "    return clean\n",
    "\n",
    "def deonehot_categoricals(df, allow_singleton=False, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Recompose des colonnes one-hot en une seule catégorie (dtype category).\n",
    "    - multi-colonnes -> fusion\n",
    "    - singleton -> fusion seulement si allow_singleton=True\n",
    "    - \"<NA>\" (texte) -> NaN, rangé en dernier\n",
    "    - exclude_cols protégées (ex: cible)\n",
    "    \"\"\"\n",
    "    groups = detect_onehot_groups(df, allow_singleton=allow_singleton, exclude_cols=exclude_cols)\n",
    "    out = df.copy()\n",
    "\n",
    "    def label_sort_key(lab):\n",
    "        return (1, \"\") if lab == \"<NA>\" else (0, str(lab))\n",
    "\n",
    "    for base, items in groups.items():\n",
    "        items_sorted = sorted(items, key=lambda x: label_sort_key(x[1]))\n",
    "\n",
    "        if len(items_sorted) == 1 and allow_singleton:\n",
    "            col, lab = items_sorted[0]\n",
    "            ser = pd.Series(\"__OTHER__\", index=df.index, dtype=\"object\")\n",
    "            mask = (df[col] == 1)\n",
    "            ser[mask] = (pd.NA if lab == \"<NA>\" else lab)\n",
    "            out[base] = ser.astype(\"category\")\n",
    "            out.drop(columns=[col], inplace=True, errors=\"ignore\")\n",
    "            continue\n",
    "\n",
    "        if len(items_sorted) >= 2:\n",
    "            cols_sorted = [c for c, _ in items_sorted]\n",
    "            labels = [lab for _, lab in items_sorted]\n",
    "            ser = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "            for c, lab in zip(cols_sorted, labels):\n",
    "                mask = (df[c] == 1)\n",
    "                ser[mask] = (pd.NA if lab == \"<NA>\" else lab)\n",
    "            out[base] = ser.astype(\"category\")\n",
    "            out.drop(columns=cols_sorted, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ======================================\n",
    "# Étape 1 — Cible binaire (auto/forcée)\n",
    "# ======================================\n",
    "def infer_binary_target(df, prefer_name_patterns=('default', 'delinq', 'bad', 'target', 'label')):\n",
    "    candidates = []\n",
    "    for col in df.columns:\n",
    "        s = df[col]\n",
    "        is_bool = pd.api.types.is_bool_dtype(s)\n",
    "        is_binary_int = (pd.api.types.is_integer_dtype(s) or pd.api.types.is_numeric_dtype(s)) and s.dropna().isin([0, 1]).all()\n",
    "        is_binary_cat = isinstance(s.dtype, pd.CategoricalDtype) and s.dropna().nunique() == 2\n",
    "        if is_bool or is_binary_int or is_binary_cat:\n",
    "            score = 0.0\n",
    "            name_lower = col.lower()\n",
    "            for p in prefer_name_patterns:\n",
    "                if p in name_lower:\n",
    "                    score += 10.0\n",
    "            try:\n",
    "                score += float(1 - min(max(float(s.astype(\"Int64\").mean(skipna=True)), 1e-6), 1 - 1e-6))\n",
    "            except Exception:\n",
    "                pass\n",
    "            candidates.append((score, col))\n",
    "    if not candidates:\n",
    "        raise ValueError(\"Aucune colonne binaire éligible trouvée pour servir de cible.\")\n",
    "    candidates.sort(reverse=True)\n",
    "    return candidates[0][1]\n",
    "\n",
    "# ===================================================\n",
    "# Étape 2 — Colonnes catégorielles brutes\n",
    "# ===================================================\n",
    "def find_categorical_columns(df, target_col=None, max_levels_object=50, exclude_ids=None):\n",
    "    exclude_ids = set(exclude_ids or [])\n",
    "    cat_cols = []\n",
    "    for col in df.columns:\n",
    "        if col == target_col or col in exclude_ids:\n",
    "            continue\n",
    "        s = df[col]\n",
    "        if isinstance(s.dtype, pd.CategoricalDtype) or pd.api.types.is_bool_dtype(s):\n",
    "            cat_cols.append(col)\n",
    "        elif pd.api.types.is_object_dtype(s) or str(s.dtype).startswith(\"string\"):\n",
    "            if s.nunique(dropna=True) <= max_levels_object:\n",
    "                cat_cols.append(col)\n",
    "        elif pd.api.types.is_integer_dtype(s) and s.nunique(dropna=True) <= 8:\n",
    "            if not any(k in col.lower() for k in ['id', 'sequence', 'loan_sequence']):\n",
    "                cat_cols.append(col)\n",
    "    return cat_cols\n",
    "\n",
    "def extract_ordinal_info(df, cat_cols):\n",
    "    ordinal_cols, explicit_orders = [], {}\n",
    "    for col in cat_cols:\n",
    "        s = df[col]\n",
    "        if isinstance(s.dtype, pd.CategoricalDtype) and getattr(s.dtype, \"ordered\", False):\n",
    "            ordinal_cols.append(col)\n",
    "            explicit_orders[col] = list(s.dtype.categories)\n",
    "    return ordinal_cols, explicit_orders\n",
    "\n",
    "# =========================================================\n",
    "# Étape 3 — Binning catégoriel (fusion pour max |Gini|)\n",
    "# =========================================================\n",
    "def _cat_stats(df, col, target_col=\"target\",\n",
    "               include_missing=True, missing_label=\"__MISSING__\"):\n",
    "    target = df[target_col].astype(int)\n",
    "    ser = df[col]\n",
    "    if include_missing:\n",
    "        ser = ser.astype(\"object\").where(ser.notna(), missing_label)\n",
    "    tmp = pd.DataFrame({col: ser, target_col: target})\n",
    "    agg = tmp.groupby(col, dropna=not include_missing)[target_col].agg([\"sum\", \"count\"])\n",
    "    agg.rename(columns={\"sum\": \"n_bad\", \"count\": \"n_total\"}, inplace=True)\n",
    "    agg[\"n_good\"] = agg[\"n_total\"] - agg[\"n_bad\"]\n",
    "    n_total = len(df)\n",
    "    n_bad = int(target.sum())\n",
    "    n_good = n_total - n_bad\n",
    "    denom_bad = n_bad if n_bad > 0 else 1\n",
    "    denom_good = n_good if n_good > 0 else 1\n",
    "    agg[\"bad_rate\"] = agg[\"n_bad\"] / agg[\"n_total\"].where(agg[\"n_total\"] > 0, 1)\n",
    "    agg[\"bad_share\"] = agg[\"n_bad\"] / denom_bad\n",
    "    agg[\"good_share\"] = agg[\"n_good\"] / denom_good\n",
    "    return agg.reset_index().rename(columns={col: \"modality\"})\n",
    "\n",
    "def _groups_df_from_bins(stats_df, bins, order_key=\"bad_rate\", ascending=True):\n",
    "    rows = []\n",
    "    for i, mods in enumerate(bins):\n",
    "        sub = stats_df[stats_df[\"modality\"].isin(mods)]\n",
    "        n_bad = int(sub[\"n_bad\"].sum())\n",
    "        n_good = int(sub[\"n_good\"].sum())\n",
    "        n_tot = int(sub[\"n_total\"].sum())\n",
    "        br = n_bad / n_tot if n_tot > 0 else 0.0\n",
    "        rows.append({\"bin_id\": i, \"modalities\": tuple(mods),\n",
    "                     \"n_total\": n_tot, \"n_bad\": n_bad, \"n_good\": n_good,\n",
    "                     \"bad_rate\": br,\n",
    "                     \"bad_share\": sub[\"bad_share\"].sum(),\n",
    "                     \"good_share\": sub[\"good_share\"].sum()})\n",
    "    gdf = pd.DataFrame(rows).sort_values(order_key, ascending=ascending, kind=\"mergesort\").reset_index(drop=True)\n",
    "    gdf[\"bad_cum\"] = gdf[\"bad_share\"].cumsum()\n",
    "    gdf[\"good_cum\"] = gdf[\"good_share\"].cumsum()\n",
    "    return gdf\n",
    "\n",
    "def _gini_from_bins(stats_df, bins, order_key=\"bad_rate\", ascending=True):\n",
    "    gdf = _groups_df_from_bins(stats_df, bins, order_key, ascending)\n",
    "    df_cum = gdf.rename(columns={\"good_cum\": \"good_client_share_cumsum\",\n",
    "                                 \"bad_cum\": \"bad_client_share_cumsum\"})[[\"good_client_share_cumsum\", \"bad_client_share_cumsum\"]]\n",
    "    return gini_trapz(df_cum, y_col=\"bad_client_share_cumsum\",\n",
    "                      x_col=\"good_client_share_cumsum\", signed=False)\n",
    "\n",
    "def _initial_order(stats_df, ordered=False, explicit_order=None, nominal_order_key=\"bad_rate\"):\n",
    "    if ordered:\n",
    "        order = list(explicit_order) if explicit_order is not None else list(stats_df[\"modality\"])\n",
    "        order = [m for m in order if m in set(stats_df[\"modality\"])] + \\\n",
    "                [m for m in stats_df[\"modality\"] if m not in set(order)]\n",
    "    else:\n",
    "        order = list(stats_df.sort_values(nominal_order_key)[\"modality\"])\n",
    "    return order\n",
    "\n",
    "def _reorder_after_merge(groups, stats_df, ordered, nominal_order_key=\"bad_rate\"):\n",
    "    if ordered:\n",
    "        return groups\n",
    "\n",
    "    def grp_bad_rate(mods):\n",
    "        sub = stats_df[stats_df[\"modality\"].isin(mods)]\n",
    "        nb, nt = sub[\"n_bad\"].sum(), sub[\"n_total\"].sum()\n",
    "        return (nb / nt) if nt > 0 else 0.0\n",
    "\n",
    "    return sorted(groups, key=lambda mods: grp_bad_rate(mods))\n",
    "\n",
    "def maximize_gini_via_merging(\n",
    "    df, col, target_col,\n",
    "    include_missing=True, missing_label=\"__MISSING__\",\n",
    "    ordered=False, explicit_order=None,\n",
    "    max_bins=6, min_bin_size=200,\n",
    "    order_key_for_curve=\"bad_rate\", nominal_order_key=\"bad_rate\"\n",
    "):\n",
    "    stats_df = _cat_stats(df, col, target_col, include_missing, missing_label)\n",
    "    order = _initial_order(stats_df, ordered, explicit_order, nominal_order_key)\n",
    "    groups = [[m] for m in order]\n",
    "    if len(groups) <= 1:\n",
    "        mapping = {m: 0 for m in order}\n",
    "        gdf_final = _groups_df_from_bins(stats_df, groups, order_key_for_curve, True)\n",
    "        g = _gini_from_bins(stats_df, groups, order_key_for_curve, True)\n",
    "        return {\"mapping\": mapping, \"gini_before\": float(g), \"gini_after\": float(g),\n",
    "                \"bins_table\": gdf_final, \"bins\": [tuple(g) for g in groups]}\n",
    "\n",
    "    # Contraintes\n",
    "    def constraints_ok(groups):\n",
    "        if max_bins is not None and len(groups) > max_bins:\n",
    "            return False\n",
    "        if min_bin_size and min_bin_size > 0:\n",
    "            for mods in groups:\n",
    "                if int(stats_df[stats_df[\"modality\"].isin(mods)][\"n_total\"].sum()) < min_bin_size:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    # Merge glouton tant que les contraintes ne sont pas respectées\n",
    "    while not constraints_ok(groups):\n",
    "        best_g, best_i = -np.inf, None\n",
    "        for i in range(len(groups) - 1):\n",
    "            merged = groups[:i] + [groups[i] + groups[i + 1]] + groups[i + 2:]\n",
    "            merged = _reorder_after_merge(merged, stats_df, ordered, nominal_order_key)\n",
    "            g_try = _gini_from_bins(stats_df, merged, order_key_for_curve, True)\n",
    "            if g_try > best_g:\n",
    "                best_g, best_i = g_try, i\n",
    "        if best_i is None:\n",
    "            best_i = 0\n",
    "        groups = groups[:best_i] + [groups[best_i] + groups[best_i + 1]] + groups[best_i + 2:]\n",
    "        groups = _reorder_after_merge(groups, stats_df, ordered, nominal_order_key)\n",
    "\n",
    "    gini_before = _gini_from_bins(stats_df, [[m] for m in order], order_key_for_curve, True)\n",
    "    gini_after = _gini_from_bins(stats_df, groups, order_key_for_curve, True)\n",
    "    final_bins = [tuple(mods) for mods in groups]\n",
    "    mapping = {m: b for b, mods in enumerate(final_bins) for m in mods}\n",
    "    gdf_final = _groups_df_from_bins(stats_df, groups, order_key_for_curve, True)\n",
    "    return {\"mapping\": mapping, \"gini_before\": float(gini_before), \"gini_after\": float(gini_after),\n",
    "            \"bins_table\": gdf_final, \"bins\": final_bins}\n",
    "\n",
    "# ==========================================================\n",
    "# Étape 4 — Binning numérique (quantiles glouton, max |Gini|)\n",
    "#            + conversions dates -> jours + edges sûres\n",
    "# ==========================================================\n",
    "def _is_period_dtype(dt):\n",
    "    try:\n",
    "        return pd.api.types.is_period_dtype(dt)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _to_float_series(s):\n",
    "    # Period -> début de période -> jours depuis epoch\n",
    "    if _is_period_dtype(s.dtype):\n",
    "        ts = s.dt.to_timestamp(how=\"start\")\n",
    "        days = (ts.view(\"int64\") // 86_400_000_000_000)  # ns -> jours\n",
    "        return days.astype(\"float64\")\n",
    "    # Datetime -> jours depuis epoch (protège les tz)\n",
    "    if pd.api.types.is_datetime64_any_dtype(s):\n",
    "        if hasattr(s.dt, \"tz\") and s.dt.tz is not None:\n",
    "            s = s.dt.tz_localize(None)\n",
    "        days = (s.astype(\"datetime64[ns]\").view(\"int64\") // 86_400_000_000_000)\n",
    "        return days.astype(\"float64\")\n",
    "    # Numérique\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return pd.to_numeric(s, errors=\"coerce\").astype(\"float64\")\n",
    "    # Objet -> numérique (coerce)\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "def _safe_edges_for_cut(edges, s_float):\n",
    "    \"\"\"\n",
    "    edges: liste triée [-inf, t1, ..., +inf] -> array strictement croissante\n",
    "    élargit extrémités et corrige les égalités numériques.\n",
    "    \"\"\"\n",
    "    e = np.array(edges, dtype=\"float64\")\n",
    "    for i in range(1, len(e)):\n",
    "        if not (e[i] > e[i - 1]):\n",
    "            e[i] = np.nextafter(e[i - 1], np.inf)\n",
    "\n",
    "    s_vals = s_float.to_numpy()\n",
    "    try:\n",
    "        s_min = float(np.nanmin(s_vals))\n",
    "        s_max = float(np.nanmax(s_vals))\n",
    "    except ValueError:\n",
    "        # tout NaN\n",
    "        s_min, s_max = -1.0, 1.0\n",
    "\n",
    "    rel_eps_lo = 1e-6 * (abs(e[1]) + 1.0) if len(e) > 1 else 1e-6\n",
    "    rel_eps_hi = 1e-6 * (abs(e[-2]) + 1.0) if len(e) > 1 else 1e-6\n",
    "    if len(e) >= 2:\n",
    "        e[0] = min(e[1] - rel_eps_lo, s_min - rel_eps_lo)\n",
    "        e[-1] = max(e[-2] + rel_eps_hi, s_max + rel_eps_hi)\n",
    "    return e\n",
    "\n",
    "def _gini_from_numeric_bins(y_int, x_float, edges, include_missing=True):\n",
    "    y = y_int.astype(int).to_numpy()\n",
    "    x = x_float.to_numpy()\n",
    "    bins_idx = np.digitize(x, edges[1:-1], right=True)\n",
    "    K = len(edges) - 1\n",
    "    n_total = len(y)\n",
    "    n_bad = int(y.sum())\n",
    "    n_good = n_total - n_bad\n",
    "    denom_bad = n_bad if n_bad > 0 else 1\n",
    "    denom_good = n_good if n_good > 0 else 1\n",
    "    rows = []\n",
    "    for k in range(K):\n",
    "        mask = (bins_idx == k) & ~np.isnan(x)\n",
    "        nk = int(mask.sum())\n",
    "        nb = int(y[mask].sum())\n",
    "        ng = nk - nb\n",
    "        br = nb / nk if nk > 0 else 0.0\n",
    "        rows.append({\"bin\": k, \"n_total\": nk, \"n_bad\": nb, \"n_good\": ng, \"bad_rate\": br})\n",
    "    if include_missing and np.isnan(x).any():\n",
    "        mask = np.isnan(x)\n",
    "        nk = int(mask.sum())\n",
    "        nb = int(y[mask].sum())\n",
    "        ng = nk - nb\n",
    "        br = nb / nk if nk > 0 else 0.0\n",
    "        rows.append({\"bin\": K, \"n_total\": nk, \"n_bad\": nb, \"n_good\": ng, \"bad_rate\": br})\n",
    "\n",
    "    gdf = pd.DataFrame(rows)\n",
    "    if gdf.empty:\n",
    "        return 0.0, gdf\n",
    "    gdf[\"bad_share\"] = gdf[\"n_bad\"] / denom_bad\n",
    "    gdf[\"good_share\"] = gdf[\"n_good\"] / denom_good\n",
    "    gdf = gdf.sort_values(\"bad_rate\").reset_index(drop=True)\n",
    "    gdf[\"bad_cum\"] = gdf[\"bad_share\"].cumsum()\n",
    "    gdf[\"good_cum\"] = gdf[\"good_share\"].cumsum()\n",
    "    df_cum = gdf.rename(columns={\"good_cum\": \"good_client_share_cumsum\",\n",
    "                                 \"bad_cum\": \"bad_client_share_cumsum\"})[[\"good_client_share_cumsum\", \"bad_client_share_cumsum\"]]\n",
    "    g = gini_trapz(df_cum, y_col=\"bad_client_share_cumsum\",\n",
    "                   x_col=\"good_client_share_cumsum\", signed=False)\n",
    "    return g, gdf\n",
    "\n",
    "def optimize_numeric_binning_by_quantiles(\n",
    "    df, col, target_col,\n",
    "    max_bins=6, min_bin_size=200,\n",
    "    n_quantiles=50, q_low=0.02, q_high=0.98,\n",
    "    include_missing=True, min_gain=1e-5\n",
    "):\n",
    "    s = _to_float_series(df[col])\n",
    "    y = df[target_col].astype(int)\n",
    "    nunique = s.dropna().nunique()\n",
    "    if nunique < 2:\n",
    "        # une seule modalité -> un bin\n",
    "        g0, _ = _gini_from_numeric_bins(y, s, [-np.inf, np.inf], include_missing)\n",
    "        return {\"edges\": [-np.inf, np.inf], \"edges_for_cut\": [-1.0, 1.0], \"labels\": [\"(-inf, inf]\"],\n",
    "                \"gini_before\": float(g0), \"gini_after\": float(g0), \"bins_table\": pd.DataFrame()}\n",
    "\n",
    "    qs = np.linspace(q_low, q_high, n_quantiles)\n",
    "    cand_vals = s.quantile(qs).dropna().unique()\n",
    "    cand_vals = np.unique(cand_vals)\n",
    "    edges = [-np.inf, np.inf]\n",
    "\n",
    "    def edges_ok(e):\n",
    "        bins_idx = np.digitize(s.to_numpy(), e[1:-1], right=True)\n",
    "        for k in range(len(e) - 1):\n",
    "            if int(((bins_idx == k) & ~np.isnan(s.to_numpy())).sum()) < min_bin_size:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    gini0, _ = _gini_from_numeric_bins(y, s, edges, include_missing)\n",
    "    best_gini = gini0\n",
    "    improved = True\n",
    "    while improved and (len(edges) - 1) < max_bins:\n",
    "        improved = False\n",
    "        best_gain = min_gain\n",
    "        best_t = None\n",
    "        g_best = best_gini\n",
    "        for t in cand_vals:\n",
    "            if t in edges:\n",
    "                continue\n",
    "            new_edges = sorted([*edges, t])\n",
    "            # ignore seuils quasi-identiques\n",
    "            if any(np.isclose(new_edges[i], new_edges[i + 1]) for i in range(len(new_edges) - 1)):\n",
    "                continue\n",
    "            if not edges_ok(new_edges):\n",
    "                continue\n",
    "            g_try, _ = _gini_from_numeric_bins(y, s, new_edges, include_missing)\n",
    "            gain = g_try - best_gini\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_t, g_best = gain, t, g_try\n",
    "        if best_t is not None:\n",
    "            edges = sorted([*edges, best_t])\n",
    "            best_gini = g_best\n",
    "            improved = True\n",
    "\n",
    "    gini_after, bins_table = _gini_from_numeric_bins(y, s, edges, include_missing)\n",
    "    e = sorted(edges)\n",
    "    e_cut = _safe_edges_for_cut(e, s)\n",
    "    labels = [f\"({e[i]}, {e[i + 1]}]\" for i in range(len(e) - 1)]\n",
    "    return {\"edges\": e, \"edges_for_cut\": e_cut, \"labels\": labels,\n",
    "            \"gini_before\": float(gini0), \"gini_after\": float(gini_after),\n",
    "            \"bins_table\": bins_table}\n",
    "\n",
    "# ==========================================================\n",
    "# Parallélisation — helpers\n",
    "# ==========================================================\n",
    "def _compute_cat_bin_result(df_small, col, target_col,\n",
    "                            include_missing, missing_label,\n",
    "                            is_ord, explicit_order,\n",
    "                            max_bins, min_bin_size,\n",
    "                            order_key_for_curve, nominal_order_key):\n",
    "    # df_small contient uniquement [col, target_col]\n",
    "    res = maximize_gini_via_merging(\n",
    "        df=df_small, col=col, target_col=target_col,\n",
    "        include_missing=include_missing, missing_label=missing_label,\n",
    "        ordered=is_ord, explicit_order=explicit_order,\n",
    "        max_bins=max_bins, min_bin_size=min_bin_size,\n",
    "        order_key_for_curve=order_key_for_curve, nominal_order_key=nominal_order_key\n",
    "    )\n",
    "    return col, res\n",
    "\n",
    "def _compute_num_bin_result(df_small, col, target_col,\n",
    "                            max_bins, min_bin_size, n_quantiles,\n",
    "                            include_missing):\n",
    "    # df_small contient uniquement [col, target_col]\n",
    "    res = optimize_numeric_binning_by_quantiles(\n",
    "        df=df_small, col=col, target_col=target_col,\n",
    "        max_bins=max_bins, min_bin_size=min_bin_size,\n",
    "        n_quantiles=n_quantiles, include_missing=include_missing\n",
    "    )\n",
    "    return col, res\n",
    "\n",
    "# ==========================================================\n",
    "# Étape 3 bis — Catégorielles (parallélisées)\n",
    "# ==========================================================\n",
    "def auto_bin_all_categoricals(\n",
    "    df, cat_columns, target_col,\n",
    "    include_missing=True, missing_label=\"__MISSING__\",\n",
    "    ordinal_cols=None, explicit_orders=None,\n",
    "    max_bins=6, min_bin_size=200,\n",
    "    order_key_for_curve=\"bad_rate\", nominal_order_key=\"bad_rate\",\n",
    "    add_binned_columns=True, bin_col_suffix=\"__BIN\",\n",
    "    n_jobs=1, verbose=0\n",
    "):\n",
    "    ordinal_cols = set(ordinal_cols or [])\n",
    "    explicit_orders = explicit_orders or {}\n",
    "    df_out = df.copy()\n",
    "    results, summary_rows = {}, []\n",
    "\n",
    "    # 1) calcule les binnings en parallèle (pas de mapping ici)\n",
    "    if n_jobs != 1 and Parallel is not None and len(cat_columns) > 0:\n",
    "        tasks = (\n",
    "            delayed(_compute_cat_bin_result)(\n",
    "                df_out[[col, target_col]].copy(),\n",
    "                col, target_col,\n",
    "                include_missing, missing_label,\n",
    "                (col in ordinal_cols), explicit_orders.get(col),\n",
    "                max_bins, min_bin_size,\n",
    "                order_key_for_curve, nominal_order_key\n",
    "            )\n",
    "            for col in cat_columns\n",
    "        )\n",
    "        out = Parallel(n_jobs=n_jobs, backend=\"loky\", verbose=verbose)(list(tasks))\n",
    "        for col, res in out:\n",
    "            results[col] = res\n",
    "    else:\n",
    "        for col in cat_columns:\n",
    "            is_ord = col in ordinal_cols\n",
    "            res = maximize_gini_via_merging(\n",
    "                df=df_out, col=col, target_col=target_col,\n",
    "                include_missing=include_missing, missing_label=missing_label,\n",
    "                ordered=is_ord, explicit_order=explicit_orders.get(col),\n",
    "                max_bins=max_bins, min_bin_size=min_bin_size,\n",
    "                order_key_for_curve=order_key_for_curve, nominal_order_key=nominal_order_key\n",
    "            )\n",
    "            results[col] = res\n",
    "\n",
    "    # 2) mapping (série, pour limiter la conso mémoire)\n",
    "    for col, res in results.items():\n",
    "        summary_rows.append({\n",
    "            \"variable\": col, \"type\": \"categorical\",\n",
    "            \"n_bins_final\": len(res[\"bins\"]),\n",
    "            \"gini_before\": res[\"gini_before\"],\n",
    "            \"gini_after\": res[\"gini_after\"],\n",
    "            \"gini_gain\": res[\"gini_after\"] - res[\"gini_before\"]\n",
    "        })\n",
    "        if add_binned_columns and col in df_out.columns:\n",
    "            ser = df_out[col].astype(\"object\")\n",
    "            if include_missing:\n",
    "                ser = ser.where(ser.notna(), missing_label)\n",
    "            df_out[col + bin_col_suffix] = ser.map(res[\"mapping\"]).astype(\"Int64\")\n",
    "\n",
    "    summary = (pd.DataFrame(summary_rows)\n",
    "               .sort_values(\"gini_after\", ascending=False)\n",
    "               .reset_index(drop=True))\n",
    "    return {\"results\": results, \"summary\": summary, \"df\": df_out}\n",
    "\n",
    "# ==========================================================\n",
    "# Étape 4 bis — Numériques (parallélisées)\n",
    "# ==========================================================\n",
    "def auto_bin_all_numerics(\n",
    "    df, target_col,\n",
    "    max_bins=6, min_bin_size=200,\n",
    "    n_quantiles=50, include_missing=True,\n",
    "    add_binned_columns=True, bin_col_suffix=\"__BIN\",\n",
    "    exclude_ids=None,\n",
    "    n_jobs=1, verbose=0\n",
    "):\n",
    "    exclude_ids = set(exclude_ids or [])\n",
    "    df_out = df.copy()\n",
    "    results, summary_rows = {}, []\n",
    "\n",
    "    # Détection colonnes numériques/period/datetime\n",
    "    numeric_cols = []\n",
    "    for col in df.columns:\n",
    "        if col == target_col or col in exclude_ids:\n",
    "            continue\n",
    "        s = df[col]\n",
    "        if pd.api.types.is_numeric_dtype(s) and not pd.api.types.is_bool_dtype(s):\n",
    "            if s.dropna().isin([0, 1]).all():\n",
    "                continue\n",
    "            if pd.api.types.is_integer_dtype(s) and s.dropna().nunique() <= 8:\n",
    "                continue\n",
    "            if any(k in col.lower() for k in ['id', 'sequence', 'postal', 'zip', 'msa', 'code', 'seller', 'servicer']):\n",
    "                continue\n",
    "            numeric_cols.append(col)\n",
    "        elif _is_period_dtype(s.dtype) or pd.api.types.is_datetime64_any_dtype(s):\n",
    "            numeric_cols.append(col)\n",
    "\n",
    "    # 1) calcule les binnings en parallèle (pas de cut ici)\n",
    "    if n_jobs != 1 and Parallel is not None and len(numeric_cols) > 0:\n",
    "        tasks = (\n",
    "            delayed(_compute_num_bin_result)(\n",
    "                df_out[[col, target_col]].copy(),\n",
    "                col, target_col,\n",
    "                max_bins, min_bin_size, n_quantiles,\n",
    "                include_missing\n",
    "            )\n",
    "            for col in numeric_cols\n",
    "        )\n",
    "        out = Parallel(n_jobs=n_jobs, backend=\"loky\", verbose=verbose)(list(tasks))\n",
    "        for col, res in out:\n",
    "            results[col] = res\n",
    "    else:\n",
    "        for col in numeric_cols:\n",
    "            res = optimize_numeric_binning_by_quantiles(\n",
    "                df=df_out, col=col, target_col=target_col,\n",
    "                max_bins=max_bins, min_bin_size=min_bin_size,\n",
    "                n_quantiles=n_quantiles, include_missing=include_missing\n",
    "            )\n",
    "            results[col] = res\n",
    "\n",
    "    # 2) application des cuts (série, mémoire friendly)\n",
    "    for col, res in results.items():\n",
    "        summary_rows.append({\n",
    "            \"variable\": col, \"type\": \"numeric\",\n",
    "            \"n_bins_final\": len(res[\"edges\"]) - 1,\n",
    "            \"gini_before\": res[\"gini_before\"],\n",
    "            \"gini_after\": res[\"gini_after\"],\n",
    "            \"gini_gain\": res[\"gini_after\"] - res[\"gini_before\"]\n",
    "        })\n",
    "        if add_binned_columns and col in df_out.columns:\n",
    "            s = _to_float_series(df_out[col])\n",
    "            b = pd.cut(s, bins=res[\"edges_for_cut\"], include_lowest=True, duplicates=\"drop\")\n",
    "            b = b.cat.codes.astype(\"Int64\")\n",
    "            if include_missing and s.isna().any():\n",
    "                b = b.where(~s.isna(), -1).astype(\"Int64\")\n",
    "            df_out[col + bin_col_suffix] = b\n",
    "\n",
    "    summary = (pd.DataFrame(summary_rows)\n",
    "               .sort_values(\"gini_after\", ascending=False)\n",
    "               .reset_index(drop=True))\n",
    "    return {\"results\": results, \"summary\": summary, \"df\": df_out}\n",
    "\n",
    "# ============================================\n",
    "# Étape 5 — Assemblage final + One-Hot des BIN\n",
    "# ============================================\n",
    "def build_final_datasets(out_cat, out_num, drop_original=True, bin_col_suffix=\"__BIN\",\n",
    "                         keep_vars=None):\n",
    "    \"\"\"\n",
    "    keep_vars: iterable de noms de variables (sans suffixe __BIN) à conserver.\n",
    "               Si None -> conserve toutes les variables binned.\n",
    "    \"\"\"\n",
    "    df_enrichi = out_num[\"df\"].copy()\n",
    "    # récupère aussi les BIN caté ajoutés dans out_cat\n",
    "    for c in out_cat[\"df\"].columns:\n",
    "        if c.endswith(bin_col_suffix) and c not in df_enrichi.columns:\n",
    "            df_enrichi[c] = out_cat[\"df\"][c]\n",
    "\n",
    "    cat_cols_all = list(out_cat[\"results\"].keys())\n",
    "    num_cols_all = list(out_num[\"results\"].keys())\n",
    "\n",
    "    if keep_vars is not None:\n",
    "        keep_vars = set(keep_vars)\n",
    "        cat_cols = [c for c in cat_cols_all if c in keep_vars]\n",
    "        num_cols = [c for c in num_cols_all if c in keep_vars]\n",
    "    else:\n",
    "        cat_cols = cat_cols_all\n",
    "        num_cols = num_cols_all\n",
    "\n",
    "    bin_cols = [c + bin_col_suffix for c in cat_cols + num_cols if c + bin_col_suffix in df_enrichi.columns]\n",
    "\n",
    "    # Dataset binned (optionnel) : on supprime les brutes\n",
    "    if drop_original:\n",
    "        df_binned = (df_enrichi\n",
    "                     .drop(columns=cat_cols_all + num_cols_all, errors=\"ignore\")\n",
    "                     .rename(columns={c: c.replace(bin_col_suffix, \"\") for c in bin_cols}))\n",
    "    else:\n",
    "        df_binned = df_enrichi.copy()\n",
    "\n",
    "    # One-Hot final uniquement des colonnes BIN retenues\n",
    "    df_ohe = pd.get_dummies(\n",
    "        df_enrichi.drop(columns=cat_cols_all + num_cols_all, errors=\"ignore\"),\n",
    "        columns=bin_cols,\n",
    "        prefix={c: c.replace(bin_col_suffix, \"\") for c in bin_cols},\n",
    "        dummy_na=False\n",
    "    )\n",
    "    # Option : ordre stable des colonnes\n",
    "    df_ohe = df_ohe.reindex(sorted(df_ohe.columns), axis=1)\n",
    "    return df_enrichi, df_binned, df_ohe\n",
    "\n",
    "# ============================================\n",
    "# Étape 6 — LANCEUR complet (protège la cible, parallélisé)\n",
    "# ============================================\n",
    "def run_full_pipeline_on_onehot_df(\n",
    "    df_onehot,\n",
    "    target_col=None,                         # passe \"default_24m\" pour forcer\n",
    "    max_bins_categ=6, min_bin_size_categ=200,\n",
    "    max_bins_num=6,   min_bin_size_num=200, n_quantiles_num=50,\n",
    "    include_missing=True, missing_label=\"__MISSING__\", max_levels_object=50,\n",
    "    bin_col_suffix=\"__BIN\",\n",
    "    exclude_ids=(\"loan_sequence_number\", \"postal_code\", \"seller_name\", \"servicer_name\", \"msa_md\"),\n",
    "    n_jobs_categ=-1, n_jobs_num=-1, verbose=0,\n",
    "    min_gini_keep=None   # ⇦ optionnel: filtre les variables à faible Gini (ex: 1e-6)\n",
    "):\n",
    "    # 0) dé-one-hot en protégeant la cible si fournie\n",
    "    DF = deonehot_categoricals(\n",
    "        df_onehot,\n",
    "        allow_singleton=False,                         # évite d'avaler des singletons ambigus\n",
    "        exclude_cols=[target_col] if target_col else None\n",
    "    )\n",
    "\n",
    "    # 1) cible\n",
    "    if target_col is not None and target_col not in DF.columns:\n",
    "        print(f\"[INFO] Colonne cible '{target_col}' introuvable après préparation. Inférence automatique...\")\n",
    "        TARGET = infer_binary_target(DF)\n",
    "    else:\n",
    "        TARGET = target_col if target_col is not None else infer_binary_target(DF)\n",
    "\n",
    "    # 2) catégorielles\n",
    "    cat_cols = find_categorical_columns(DF, target_col=TARGET, max_levels_object=max_levels_object,\n",
    "                                        exclude_ids=exclude_ids)\n",
    "    ordinal_cols, explicit_orders = extract_ordinal_info(DF, cat_cols)\n",
    "    out_cat = auto_bin_all_categoricals(\n",
    "        df=DF, cat_columns=cat_cols, target_col=TARGET,\n",
    "        include_missing=include_missing, missing_label=missing_label,\n",
    "        ordinal_cols=ordinal_cols, explicit_orders=explicit_orders,\n",
    "        max_bins=max_bins_categ, min_bin_size=min_bin_size_categ,\n",
    "        order_key_for_curve=\"bad_rate\", nominal_order_key=\"bad_rate\",\n",
    "        add_binned_columns=True, bin_col_suffix=bin_col_suffix,\n",
    "        n_jobs=n_jobs_categ, verbose=verbose\n",
    "    )\n",
    "\n",
    "    # 3) numériques\n",
    "    out_num = auto_bin_all_numerics(\n",
    "        df=out_cat[\"df\"], target_col=TARGET,\n",
    "        max_bins=max_bins_num, min_bin_size=min_bin_size_num,\n",
    "        n_quantiles=n_quantiles_num, include_missing=include_missing,\n",
    "        add_binned_columns=True, bin_col_suffix=bin_col_suffix,\n",
    "        exclude_ids=exclude_ids,\n",
    "        n_jobs=n_jobs_num, verbose=verbose\n",
    "    )\n",
    "\n",
    "    # 4) datasets finaux (+ filtrage Gini optionnel)\n",
    "    summary = (pd.concat([out_cat[\"summary\"], out_num[\"summary\"]], ignore_index=True)\n",
    "               .sort_values([\"type\", \"gini_after\"], ascending=[True, False])\n",
    "               .reset_index(drop=True))\n",
    "    keep_vars = None\n",
    "    if min_gini_keep is not None:\n",
    "        keep_vars = summary.loc[summary[\"gini_after\"] >= float(min_gini_keep), \"variable\"].tolist()\n",
    "        if verbose:\n",
    "            nb_drop = (summary[\"gini_after\"] < float(min_gini_keep)).sum()\n",
    "            print(f\"[INFO] min_gini_keep={min_gini_keep} -> exclusion de {nb_drop} variables.\")\n",
    "\n",
    "    df_enrichi, df_binned, df_ohe = build_final_datasets(\n",
    "        out_cat, out_num,\n",
    "        drop_original=True,\n",
    "        bin_col_suffix=bin_col_suffix,\n",
    "        keep_vars=keep_vars\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"target\": TARGET,\n",
    "        \"summary\": summary,\n",
    "        \"df_enrichi\": df_enrichi,    # contient les colonnes *_BIN\n",
    "        \"df_binned\": df_binned,      # (optionnel) DF avec colonnes BIN renommées\n",
    "        \"df_ohe\": df_ohe,            # OHE final prêt pour le modèle\n",
    "        \"cat_results\": out_cat[\"results\"],\n",
    "        \"num_results\": out_num[\"results\"]\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# Étape 7 — Transformer val/test avec bins appris (protège la cible)\n",
    "# ============================================\n",
    "def transform_with_learned_bins(df_raw_onehot, res, bin_col_suffix=\"__BIN\",\n",
    "                                include_missing=True,\n",
    "                                exclude_ids=(\"loan_sequence_number\", \"postal_code\", \"seller_name\", \"servicer_name\", \"msa_md\")):\n",
    "    DF = deonehot_categoricals(\n",
    "        df_raw_onehot,\n",
    "        allow_singleton=False,\n",
    "        exclude_cols=[res[\"target\"]]  # protège la colonne cible\n",
    "    )\n",
    "\n",
    "    # 1) catégorielles (mappings appris)\n",
    "    for col, r in res[\"cat_results\"].items():\n",
    "        if col not in DF.columns:\n",
    "            continue\n",
    "        s = DF[col].astype(\"object\").where(DF[col].notna(), \"__MISSING__\")\n",
    "        mapped = s.map(r[\"mapping\"]).astype(\"Int64\")\n",
    "        mapped = mapped.fillna(-2).astype(\"Int64\")  # catégories jamais vues -> -2\n",
    "        DF[col + bin_col_suffix] = mapped\n",
    "\n",
    "    # 2) numériques (edges appris)\n",
    "    for col, r in res[\"num_results\"].items():\n",
    "        if col not in DF.columns:\n",
    "            continue\n",
    "        s = _to_float_series(DF[col])\n",
    "        e = np.array(r[\"edges_for_cut\"], dtype=\"float64\")\n",
    "        b = pd.cut(s, bins=e, include_lowest=True, duplicates=\"drop\")\n",
    "        b = b.cat.codes.astype(\"Int64\")\n",
    "        if include_missing and s.isna().any():\n",
    "            b = b.where(~s.isna(), -1).astype(\"Int64\")\n",
    "        DF[col + bin_col_suffix] = b\n",
    "\n",
    "    # 3) One-hot final des colonnes BIN\n",
    "    cat_cols = list(res[\"cat_results\"].keys())\n",
    "    num_cols = list(res[\"num_results\"].keys())\n",
    "    bin_cols = [c + bin_col_suffix for c in cat_cols + num_cols if c + bin_col_suffix in DF.columns]\n",
    "\n",
    "    df_model = pd.get_dummies(\n",
    "        DF.drop(columns=cat_cols + num_cols, errors=\"ignore\"),\n",
    "        columns=bin_cols,\n",
    "        prefix={c: c.replace(bin_col_suffix, \"\") for c in bin_cols},\n",
    "        dummy_na=False\n",
    "    )\n",
    "    # Option : ordre stable\n",
    "    df_model = df_model.reindex(sorted(df_model.columns), axis=1)\n",
    "    # Retire IDs\n",
    "    df_model = df_model.drop(columns=[c for c in exclude_ids if c in df_model.columns], errors=\"ignore\")\n",
    "    return df_model\n",
    "\n",
    "# ============================================\n",
    "# Étape 8 — Plots des courbes (départ à 0,0)\n",
    "# ============================================\n",
    "def _curve_from_binned(df, bcol, target):\n",
    "    y = df[target].astype(int)\n",
    "    s = df[bcol].astype(\"Int64\").fillna(-1).astype(\"int64\")\n",
    "\n",
    "    agg = pd.DataFrame({bcol: s, target: y}).groupby(bcol)[target].agg([\"sum\", \"count\"])\n",
    "    agg.columns = [\"n_bad\", \"n_total\"]\n",
    "    agg[\"n_good\"] = agg[\"n_total\"] - agg[\"n_bad\"]\n",
    "\n",
    "    n_bad = int(agg[\"n_bad\"].sum())\n",
    "    n_good = int(agg[\"n_good\"].sum())\n",
    "    if n_bad == 0 or n_good == 0:\n",
    "        df_cum = pd.DataFrame({\"good_client_share_cumsum\": [0.0, 1.0],\n",
    "                               \"bad_client_share_cumsum\": [0.0, 1.0]})\n",
    "        return df_cum, 0.0\n",
    "\n",
    "    agg[\"bad_rate\"] = agg[\"n_bad\"] / agg[\"n_total\"].where(agg[\"n_total\"] > 0, 1)\n",
    "    agg[\"bad_share\"] = agg[\"n_bad\"] / n_bad\n",
    "    agg[\"good_share\"] = agg[\"n_good\"] / n_good\n",
    "\n",
    "    agg = agg.sort_values(\"bad_rate\", kind=\"mergesort\")\n",
    "    good_cum = np.r_[0.0, agg[\"good_share\"].cumsum().values]\n",
    "    bad_cum = np.r_[0.0, agg[\"bad_share\"].cumsum().values]\n",
    "\n",
    "    df_cum = pd.DataFrame({\"good_client_share_cumsum\": good_cum,\n",
    "                           \"bad_client_share_cumsum\": bad_cum})\n",
    "    g = gini_trapz(df_cum, signed=False)\n",
    "    return df_cum, float(g)\n",
    "\n",
    "def plot_all_concentration_curves_from_binned(res, top_n=None, types=(\"categorical\", \"numeric\")):\n",
    "    df_base = res[\"df_enrichi\"]  # contient *_BIN\n",
    "    target = res[\"target\"]\n",
    "\n",
    "    # calcul des courbes\n",
    "    rows = []\n",
    "    for t, store in ((\"categorical\", res[\"cat_results\"]), (\"numeric\", res[\"num_results\"])):\n",
    "        if t not in types:\n",
    "            continue\n",
    "        for var, info in store.items():\n",
    "            bcol = f\"{var}__BIN\"\n",
    "            if bcol not in df_base.columns:\n",
    "                continue\n",
    "            df_cum, g = _curve_from_binned(df_base, bcol, target)\n",
    "            rows.append((t, var, g, df_cum))\n",
    "    rows.sort(key=lambda x: x[2], reverse=True)\n",
    "    if top_n is not None:\n",
    "        rows = rows[:int(top_n)]\n",
    "\n",
    "    # plots\n",
    "    for t, var, g, df_cum in rows:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot(df_cum[\"good_client_share_cumsum\"], df_cum[\"bad_client_share_cumsum\"], marker=\"o\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle=\"--\")  # pas de couleur spécifique\n",
    "        plt.title(f\"{var} [{t}] — Gini = {g:.4f}\")\n",
    "        plt.xlabel(\"Cumulative good share\")\n",
    "        plt.ylabel(\"Cumulative bad share\")\n",
    "        plt.grid(True)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"{t} — {var}: Gini = {g:.6f}, nb_points={len(df_cum)}\")\n",
    "\n",
    "# =======================\n",
    "# Exemple d'utilisation :\n",
    "# =======================\n",
    "\n",
    "# 1) Fit sur train (DF \"one-hot\" ou mélange)\n",
    "res = run_full_pipeline_on_onehot_df(\n",
    "    df_onehot=df_train_imp,\n",
    "    target_col=\"default_24m\",   # 👈 sera PROTÉGÉ du dé-one-hot\n",
    "    max_bins_categ=6, min_bin_size_categ=200,\n",
    "    max_bins_num=6,   min_bin_size_num=200, n_quantiles_num=50,\n",
    "    include_missing=True, missing_label=\"__MISSING__\", bin_col_suffix=\"__BIN\",\n",
    "    n_jobs_categ=-1, n_jobs_num=-1,  # ← utilise tous les cœurs disponibles\n",
    "    verbose=10,\n",
    "    # min_gini_keep=1e-6,        # ← décommente pour exclure auto les variables trop faibles\n",
    ")\n",
    "\n",
    "# 2) Jeu final pour le modèle (X, y)\n",
    "cols_id = [\"loan_sequence_number\", \"postal_code\", \"seller_name\", \"servicer_name\", \"msa_md\"]\n",
    "df_final = res[\"df_ohe\"].drop(columns=[c for c in cols_id if c in res[\"df_ohe\"].columns], errors=\"ignore\")\n",
    "y_train = df_final.pop(res[\"target\"]).astype(int)\n",
    "X_train = df_final\n",
    "\n",
    "# 3) Transformer validation/test avec les bins appris\n",
    "df_val_final = transform_with_learned_bins(df_val_imp, res)\n",
    "y_val = df_val_final.pop(res[\"target\"]).astype(int)\n",
    "X_val = df_val_final.reindex(columns=X_train.columns, fill_value=0)  # aligne les colonnes\n",
    "\n",
    "# 4) (Optionnel) Plots des 30 meilleures courbes\n",
    "plot_all_concentration_curves_from_binned(res, top_n=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd-calibration-NMoSp0fs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
