{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd52872",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbba943",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0838dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f36a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "\n",
    "\n",
    "## Non imputed Data \n",
    "df_train = pd.read_parquet(\"../data/processed/merged/non_imputed/train.parquet\")\n",
    "df_val = pd.read_parquet(\"../data/processed/merged/non_imputed/validation.parquet\")\n",
    "\n",
    "df_train.drop(columns=\"pre_relief_refi_loan_seq_number\", inplace=True)\n",
    "df_val.drop(columns=\"pre_relief_refi_loan_seq_number\", inplace=True)\n",
    "\n",
    "## Imputed data\n",
    "df_train_imp = pd.read_parquet(\"../data/processed/merged/imputed/train.parquet\")\n",
    "df_val_imp = pd.read_parquet(\"../data/processed/merged/imputed/validation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b25a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_imp[\"defa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce662d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrélation cell \n",
    "\n",
    "\n",
    "\n",
    "# MAP each column into a data types categories (to better select columns)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dict_types = {\n",
    " 'credit_score': 'continuous',\n",
    " 'first_payment_date': 'date',\n",
    " 'first_time_homebuyer_flag': 'bool',            # Y/N, 9 -> NA\n",
    " 'maturity_date': 'date',\n",
    " 'msa_md': 'category',\n",
    " 'mi_percent': 'continuous',                     # pas catégoriel\n",
    " 'number_of_units': 'category',                  # 1–4, 99 -> NA (caté OK)\n",
    " 'occupancy_status': 'category',\n",
    " 'original_cltv': 'continuous',\n",
    " 'original_dti': 'continuous',\n",
    " 'original_upb': 'continuous',\n",
    " 'original_ltv': 'continuous',\n",
    " 'original_interest_rate': 'continuous',\n",
    " 'channel': 'category',\n",
    " 'ppm_flag': 'bool',                             # Y/N\n",
    " 'amortization_type': 'category',                # FRM/ARM\n",
    " 'property_state': 'category',\n",
    " 'property_type': 'category',\n",
    " 'postal_code': 'category',\n",
    " 'loan_sequence_number': 'id',\n",
    " 'loan_purpose': 'category',\n",
    " 'original_loan_term': 'integer',                # nb de mois, pas une caté\n",
    " 'number_of_borrowers': 'integer',               # petit entier\n",
    " 'seller_name': 'category',\n",
    " 'servicer_name': 'category',\n",
    " 'super_conforming_flag': 'bool',                # Y / vide\n",
    " 'pre_relief_refi_loan_seq_number': 'id_ref',    # clé de ref, pas une caté\n",
    " 'special_eligibility_program': 'category',      # H/F/R/9\n",
    " 'relief_refinance_indicator': 'bool',           # Y / vide\n",
    " 'property_valuation_method': 'category',        # codes 1–4, 9 -> NA\n",
    " 'interest_only_indicator': 'bool',              # Y/N\n",
    " 'mi_cancellation_indicator': 'category',        # Y/N/7/9\n",
    " 'default_24m': 'bool',\n",
    " 'vintage': 'date'                               # ex. YYYY-Q à partir de FIRST_PAYMENT_DATE\n",
    "}\n",
    "\n",
    "# inversion type -> liste de colonnes\n",
    "_dict_types_col = defaultdict(list)\n",
    "for col, t in dict_types.items():\n",
    "    _dict_types_col[t].append(col)\n",
    "\n",
    "dict_types_col = dict(_dict_types_col)  # (optionnel) convertir en dict standard\n",
    "\n",
    "cont_cols  = dict_types_col.get('continuous', [])\n",
    "ord_cols   = dict_types_col.get('integer', [])      # ici: original_loan_term, number_of_borrowers (traités comme ordinal/binaire)\n",
    "bools_cols = dict_types_col.get('bool', [])\n",
    "cat_cols  = dict_types_col.get('category', []) + bools_cols\n",
    "\n",
    "\n",
    "\n",
    "# Correlations cell\n",
    "\n",
    "from scipy.stats import spearmanr, pointbiserialr, pearsonr, kendalltau\n",
    "from scipy.stats import chi2_contingency\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "spearman_pairs  = {}\n",
    "pearson_pairs   = {}\n",
    "cramer_pairs    = {}\n",
    "pb_pairs        = {}\n",
    "kendall_pairs   = {}\n",
    "\n",
    "# df_corr = df_train_imp\n",
    "df_corr = df_train\n",
    "# 1) Spearman : ordinal <-> continu (monotone)\n",
    "for x in ord_cols:\n",
    "    for y in cont_cols:\n",
    "        s = df_corr[[x, y]].dropna()\n",
    "        if s.empty: continue\n",
    "        r, p = spearmanr(s[x], s[y])\n",
    "        spearman_pairs[(x, y)] = (r, p)\n",
    "\n",
    "# 2) Pearson : continu <-> continu (sans doublons ni diagonale)\n",
    "for x, y in combinations(cont_cols, 2):\n",
    "    s = df_corr[[x, y]].dropna()\n",
    "    if s.empty: continue\n",
    "    r, p = pearsonr(s[x], s[y])\n",
    "    pearson_pairs[(x, y)] = (r, p)\n",
    "\n",
    "# 3) Point-bisérial : continu <-> booléen binaire\n",
    "bin_cols = [c for c in dict_types_col.get('bool', [])\n",
    "            if set(df_corr[c].dropna().unique()).issubset({0,1,True,False})]\n",
    "for x in cont_cols:\n",
    "    for b in bin_cols:\n",
    "        s = df_corr[[x, b]].dropna()\n",
    "        if s.empty: continue\n",
    "        r, p = pointbiserialr(s[x], s[b].astype(int))\n",
    "        pb_pairs[(x, b)] = (r, p)   # <-- bug corrigé\n",
    "\n",
    "# 4) Kendall tau-b : ordinal <-> ordinal (et en option ordinal <-> continu)\n",
    "for x, y in combinations(ord_cols, 2):\n",
    "    s = df_corr[[x, y]].dropna()\n",
    "    if s.empty: continue\n",
    "    tau, p = kendalltau(s[x], s[y], method=\"auto\")  # tau-b avec correction des ties\n",
    "    kendall_pairs[(x, y)] = (tau, p)\n",
    "\n",
    "# Optionnel : Kendall pour ordinal <-> continu\n",
    "for x in ord_cols:\n",
    "    for y in cont_cols:\n",
    "        s = df_corr[[x, y]].dropna()\n",
    "        if s.empty: continue\n",
    "        tau, p = kendalltau(s[x], s[y], method=\"auto\")\n",
    "        kendall_pairs[(x, y)] = (tau, p)\n",
    "\n",
    "# 5) Cramér V corrigé : nominal/ordinal <-> nominal/ordinal\n",
    "def cramers_v_corrected(x, y):\n",
    "    tbl = pd.crosstab(x, y)\n",
    "    if tbl.size == 0: \n",
    "        return np.nan\n",
    "    chi2 = chi2_contingency(tbl, correction=False)[0]\n",
    "    n = tbl.values.sum()\n",
    "    r, k = tbl.shape\n",
    "    phi2 = chi2 / n\n",
    "    # Correction biais (Bergsma, 2013)\n",
    "    phi2corr = max(0, phi2 - (k-1)*(r-1)/(n-1))\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    denom = max(1e-12, min((kcorr-1), (rcorr-1)))\n",
    "    return np.sqrt(phi2corr / denom)\n",
    "\n",
    "cats = list(set(cat_cols) | set(ord_cols))  # traiter l'ordinal comme catégoriel ici\n",
    "for a, b in combinations(cats, 2):\n",
    "    s = df_corr[[a, b]].dropna()\n",
    "    if s.empty: continue\n",
    "    cramer_pairs[(a, b)] = cramers_v_corrected(s[a], s[b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd07d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape the correlation into a long data frame\n",
    "\n",
    "\n",
    "# ---------- 1) Fusion des dictionnaires en un tableau \"long\" ----------\n",
    "def _dict_to_df(d, measure, has_p=True):\n",
    "    rows = []\n",
    "    for (x, y), val in d.items():\n",
    "        # tolère (stat, p) ou (stat, p, n)\n",
    "        if has_p:\n",
    "            stat = val[0]\n",
    "            pval = val[1]\n",
    "            n = val[2] if (isinstance(val, (tuple, list)) and len(val) >= 3) else np.nan\n",
    "        else:\n",
    "            stat = val\n",
    "            pval = np.nan\n",
    "            n = np.nan\n",
    "        rows.append({\n",
    "            \"var_x\": x,\n",
    "            \"var_y\": y,\n",
    "            \"measure\": measure,\n",
    "            \"stat\": float(stat),\n",
    "            \"p_value\": float(pval) if pd.notna(pval) else np.nan,\n",
    "            \"n\": float(n) if pd.notna(n) else np.nan,\n",
    "            \"abs_stat\": abs(float(stat)),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "dfs = []\n",
    "if len(spearman_pairs): dfs.append(_dict_to_df(spearman_pairs, \"spearman\", has_p=True))\n",
    "if len(pearson_pairs):  dfs.append(_dict_to_df(pearson_pairs,  \"pearson\",  has_p=True))\n",
    "if len(pb_pairs):       dfs.append(_dict_to_df(pb_pairs,       \"pointbiserial\", has_p=True))\n",
    "if len(kendall_pairs):  dfs.append(_dict_to_df(kendall_pairs,  \"kendall_tau\", has_p=True))\n",
    "if len(cramer_pairs):   dfs.append(_dict_to_df(cramer_pairs,   \"cramers_v\", has_p=False))\n",
    "\n",
    "assocs_long = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame(\n",
    "    columns=[\"var_x\",\"var_y\",\"measure\",\"stat\",\"p_value\",\"n\",\"abs_stat\"]\n",
    ")\n",
    "\n",
    "# ---------- 2) Ajout des types de variables (utile pour filtrer) ----------\n",
    "def _type_of(v):\n",
    "    if v in bin_cols:  return \"bin\"\n",
    "    if v in cont_cols: return \"cont\"\n",
    "    if v in ord_cols:  return \"ord\"\n",
    "    if v in cat_cols:  return \"cat\"\n",
    "    return \"unknown\"\n",
    "\n",
    "assocs_long[\"type_x\"] = assocs_long[\"var_x\"].map(_type_of)\n",
    "assocs_long[\"type_y\"] = assocs_long[\"var_y\"].map(_type_of)\n",
    "\n",
    "# ---------- 3) Tri + étoiles de significativité + q-values (FDR BH) ----------\n",
    "def _stars(p):\n",
    "    if pd.isna(p): return \"\"\n",
    "    return \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "\n",
    "assocs_long[\"sig\"] = assocs_long[\"p_value\"].apply(_stars)\n",
    "\n",
    "# FDR Benjamini–Hochberg (si statsmodels dispo)\n",
    "try:\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    m = assocs_long[\"p_value\"].notna()\n",
    "    assocs_long.loc[m, \"q_value\"] = multipletests(assocs_long.loc[m, \"p_value\"], method=\"fdr_bh\")[1]\n",
    "except Exception:\n",
    "    assocs_long[\"q_value\"] = np.nan\n",
    "\n",
    "assocs_long = assocs_long.sort_values([\"measure\", \"abs_stat\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# ---------- 4) Exemples d’usages rapides ----------\n",
    "# a) Top 20 associations par mesure (en valeur absolue)\n",
    "top20_by_measure = (\n",
    "    assocs_long.groupby(\"measure\", group_keys=False)\n",
    "               .apply(lambda g: g.nlargest(20, \"abs_stat\"))\n",
    "               .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# b) Pivots pour heatmaps (ex: Pearson et Cramér V)\n",
    "pearson_pivot = (assocs_long.query(\"measure == 'pearson'\")\n",
    "                              .pivot(index=\"var_x\", columns=\"var_y\", values=\"stat\"))\n",
    "cramers_pivot = (assocs_long.query(\"measure == 'cramers_v'\")\n",
    "                              .pivot(index=\"var_x\", columns=\"var_y\", values=\"stat\"))\n",
    "\n",
    "# ---------- 5) Export CSV ----------\n",
    "assocs_long.to_csv(\"associations_long.csv\", index=False)\n",
    "top20_by_measure.to_csv(\"associations_top20_by_measure.csv\", index=False)\n",
    "\n",
    "print(\"OK • assocs_long :\", assocs_long.shape, \"| Sauvé: associations_long.csv\")\n",
    "assocs_long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72df34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_threshold = {\n",
    "    \"spearman\": 0.70,\n",
    "    \"pearson\": 0.70,\n",
    "    \"pointbiserial\": 0.70,\n",
    "    \"kendall_tau\": 0.60,\n",
    "    \"cramers_v\": 0.60\n",
    "}\n",
    "\n",
    "pair_var = pd.DataFrame(columns=assocs_long.columns)\n",
    "\n",
    "for k, v in dict_threshold.items():\n",
    "    mask = (\n",
    "        (assocs_long[\"measure\"] == k) &\n",
    "        (assocs_long[\"stat\"].notna()) &\n",
    "        (assocs_long[\"stat\"] >= v)\n",
    "    )\n",
    "    concat_df = assocs_long[mask]\n",
    "    pair_var = pd.concat([pair_var, concat_df], ignore_index=True)\n",
    "\n",
    "print(pair_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96aeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "# =========================\n",
    "# 0) Préparation\n",
    "# =========================\n",
    "TARGET = \"default_24m\"\n",
    "\n",
    "df = df_train_imp.copy()\n",
    "\n",
    "# Règles métier de drop initial\n",
    "initial_drop = set()\n",
    "\n",
    "# IDs\n",
    "initial_drop.add(\"loan_sequence_number\")\n",
    "\n",
    "# Dates -> on garde vintage par défaut\n",
    "if \"vintage\" in df.columns:\n",
    "    for c in [\"first_payment_date\", \"maturity_date\"]:\n",
    "        if c in df.columns:\n",
    "            initial_drop.add(c)\n",
    "\n",
    "# CLTV vs LTV : garder LTV par défaut\n",
    "if \"original_cltv\" in df.columns and \"original_ltv\" in df.columns:\n",
    "    initial_drop.add(\"original_cltv\")\n",
    "    # si tu drop CLTV, l'indicateur de NA associé ne sert plus\n",
    "    if \"cltv_missing\" in df.columns:\n",
    "        initial_drop.add(\"cltv_missing\")\n",
    "\n",
    "# Géographie hiérarchique : drop postal_code et msa_md (par défaut)\n",
    "for c in [\"postal_code\", \"msa_md\"]:\n",
    "    if c in df.columns:\n",
    "        initial_drop.add(c)\n",
    "\n",
    "# Dérivés évidents\n",
    "if \"has_mi\" in df.columns and \"mi_percent\" in df.columns:\n",
    "    initial_drop.add(\"has_mi\")\n",
    "if \"has_special_program\" in df.columns:\n",
    "    initial_drop.add(\"has_special_program\")\n",
    "\n",
    "# Applique drop initial\n",
    "keep_cols = [c for c in df.columns if c not in initial_drop and c != TARGET]\n",
    "X0 = df[keep_cols].copy()\n",
    "y = df[TARGET].astype(int).values\n",
    "\n",
    "# =========================\n",
    "# 1) Scoring univarié par logit CV\n",
    "# =========================\n",
    "def score_univar_cv(X, y, n_splits=5, random_state=42):\n",
    "    scores = {}\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for col in X.columns:\n",
    "        xcol = X[[col]]\n",
    "        # Détermine type\n",
    "        is_num = xcol.dtypes.iloc[0].kind in \"bifc\"\n",
    "        # Pipeline adapté\n",
    "        if is_num:\n",
    "            pipe = make_pipeline(\n",
    "                StandardScaler(with_mean=False),  # robuste aux constantes/sparse\n",
    "                LogisticRegression(solver=\"liblinear\", max_iter=300)\n",
    "            )\n",
    "        else:\n",
    "            pre = ColumnTransformer(\n",
    "                [(\"oh\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [col])],\n",
    "                remainder=\"drop\",\n",
    "                verbose_feature_names_out=False\n",
    "            )\n",
    "            pipe = make_pipeline(pre, LogisticRegression(solver=\"liblinear\", max_iter=300))\n",
    "\n",
    "        # P(y=1) via CV (no leakage)\n",
    "        p = cross_val_predict(pipe, xcol, y, cv=skf, method=\"predict_proba\")[:, 1]\n",
    "        # AUC / LogLoss\n",
    "        auc = roc_auc_score(y, p)\n",
    "        ll  = log_loss(y, p)\n",
    "        # Taux de NA & cardinalité\n",
    "        na_rate = xcol[col].isna().mean()\n",
    "        card = xcol[col].nunique(dropna=True)\n",
    "\n",
    "        scores[col] = {\"auc\": auc, \"logloss\": ll, \"na_rate\": na_rate, \"cardinality\": card, \"is_numeric\": is_num}\n",
    "\n",
    "    return pd.DataFrame(scores).T.reset_index().rename(columns={\"index\": \"variable\"})\n",
    "\n",
    "univar = score_univar_cv(X0, y)\n",
    "\n",
    "# =========================\n",
    "# 2) Recommandations de drop dans les paires corrélées\n",
    "#     (à partir de ton pair_var calculé auparavant)\n",
    "# =========================\n",
    "# Hypothèse : pair_var contient var_x, var_y, measure, stat (seuil déjà appliqué)\n",
    "# On merge les scores univariés pour comparer\n",
    "pv = pair_var.copy()\n",
    "pv = pv.merge(univar.add_prefix(\"x_\"), left_on=\"var_x\", right_on=\"x_variable\", how=\"left\")\n",
    "pv = pv.merge(univar.add_prefix(\"y_\"), left_on=\"var_y\", right_on=\"y_variable\", how=\"left\")\n",
    "\n",
    "def choose_drop(row, auc_eps=0.005, ll_eps=0.001):\n",
    "    # Priorité 1 : AUC (plus haut = mieux)\n",
    "    ax, ay = row[\"x_auc\"], row[\"y_auc\"]\n",
    "    lx, ly = row[\"x_logloss\"], row[\"y_logloss\"]\n",
    "    nx, ny = row[\"x_na_rate\"], row[\"y_na_rate\"]\n",
    "    cx, cy = row[\"x_cardinality\"], row[\"y_cardinality\"]\n",
    "\n",
    "    # Si AUC très proche, regarde logloss (plus bas = mieux)\n",
    "    if pd.notna(ax) and pd.notna(ay):\n",
    "        if ax > ay + auc_eps:\n",
    "            return row[\"var_y\"]  # drop y (x meilleur)\n",
    "        elif ay > ax + auc_eps:\n",
    "            return row[\"var_x\"]  # drop x (y meilleur)\n",
    "        else:\n",
    "            # AUC ~ égalité -> compare logloss si dispo\n",
    "            if pd.notna(lx) and pd.notna(ly):\n",
    "                if lx + ll_eps < ly:\n",
    "                    return row[\"var_y\"]\n",
    "                elif ly + ll_eps < lx:\n",
    "                    return row[\"var_x\"]\n",
    "            # Sinon, compare propreté puis simplicité\n",
    "            if pd.notna(nx) and pd.notna(ny) and nx != ny:\n",
    "                return row[\"var_x\"] if nx > ny else row[\"var_y\"]  # drop celle qui a + de NA\n",
    "            if pd.notna(cx) and pd.notna(cy) and cx != cy:\n",
    "                return row[\"var_x\"] if cx > cy else row[\"var_y\"]  # drop la + complexe (cardinalité)\n",
    "            # Par défaut: drop var_y\n",
    "            return row[\"var_y\"]\n",
    "\n",
    "    # Si AUC manquante d'un côté, garde celle qui a une AUC non nulle\n",
    "    if pd.notna(ax) and pd.isna(ay):\n",
    "        return row[\"var_y\"]\n",
    "    if pd.isna(ax) and pd.notna(ay):\n",
    "        return row[\"var_x\"]\n",
    "\n",
    "    # Dernier recours : cardinalité / NA\n",
    "    if pd.notna(cx) and pd.notna(cy) and cx != cy:\n",
    "        return row[\"var_x\"] if cx > cy else row[\"var_y\"]\n",
    "    if pd.notna(nx) and pd.notna(ny) and nx != ny:\n",
    "        return row[\"var_x\"] if nx > ny else row[\"var_y\"]\n",
    "\n",
    "    return row[\"var_y\"]\n",
    "\n",
    "pv[\"drop_reco\"] = pv.apply(choose_drop, axis=1)\n",
    "\n",
    "# =========================\n",
    "# 3) Liste finale des drops (règles + reco)\n",
    "# =========================\n",
    "to_drop = set(initial_drop) | set(pv[\"drop_reco\"].dropna().tolist())\n",
    "\n",
    "sorted(to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ca70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_var = pair_var.merge(univar_scores, left_on=\"var_x\", right_on=\"variable\", how=\"left\") \\\n",
    "                   .rename(columns={\"auc\":\"auc_x\", \"logloss\":\"logloss_x\"}) \\\n",
    "                   .drop(columns=\"variable\")\n",
    "\n",
    "pair_var = pair_var.merge(univar_scores, left_on=\"var_y\", right_on=\"variable\", how=\"left\") \\\n",
    "                   .rename(columns={\"auc\":\"auc_y\", \"logloss\":\"logloss_y\"}) \\\n",
    "                   .drop(columns=\"variable\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd-calibration-NMoSp0fs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
